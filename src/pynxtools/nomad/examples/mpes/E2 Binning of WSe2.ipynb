{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning raw Multidimensional Photoemission Spectroscopy (MPES) data and converting it into the NeXus format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to generate xarray based h5 files from WSe2 trARPES measurement data as detailed in this [paper](https://www.nature.com/articles/s41597-020-00769-8) and how to generate a file in the standardised [MPES NeXus format](https://manual.nexusformat.org/classes/contributed_definitions/NXmpes.html#nxmpes) from it.\n",
    "Due to the size of the example file (~6GB) you need at least 40 GB of memory on your computer you're executing this example on. If you just want to have a look on how to convert a pre-binned xarray based h5 file into the NeXus format you may have a look at the simpler [Convert to NeXus example](./E1%20Convert%20to%20NeXus.ipynb), which has lower hardware requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download RAW data (trARPES data of WSe2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we just set the main file folder for holding the measurement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FDIR = f'{os.getcwd()}/Scan049_1'\n",
    "ECAL = f'{os.getcwd()}/energycal_2019_01_08'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the provided measurement files are rather large (~6GB), they are not directly provided with the example.\n",
    "You can [download](https://zenodo.org/record/6369728/files/WSe2.zip) it from zenodo. This may take some time. Place the file in the directory of this notebook afterwards. Under Linux, macOS and in a NORTH container you can directly use the cell below to download the file with curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -o WSe2.zip \"https://zenodo.org/records/6369728/files/WSe2.zip\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the measurement files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip WSe2.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning of measurement data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary packages. For a manual on how to install this dependencies refer to the provided [INSTALL.md](./INSTALL.md) file. If you're running a pre-built docker container or working with the NORTH tools, these dependencies are already available for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpes import base as base, fprocessing as fp, analysis as aly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from dask import compute\n",
    "import datetime as dt\n",
    "import h5py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data binning for distortion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parp = fp.parallelHDF5Processor(folder=FDIR)\n",
    "parp.gather(identifier=r'/*.h5', file_sorting=True)\n",
    "len(parp.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin a small range of of files to create a momentum map for distortion correction\n",
    "parp.files = parp.files[0:50]\n",
    "axes = ['X', 'Y', 't']\n",
    "# Important to keep the whole detector area for the initial binning!\n",
    "bins = [512, 512, 300]\n",
    "ranges = [(0, 2048), (0, 2048), (64000, 68000)]\n",
    "parp.parallelBinning(axes=axes, nbins=bins, ranges=ranges, scheduler='threads', ret=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine correction landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an energy slice at the valence band maximum, containing the 6 K-points and the Gamma point as distinct features\n",
    "mc = aly.MomentumCorrector(parp.combinedresult['binned'])\n",
    "mc.selectSlice2D(slice(165, 175), 2)\n",
    "# Extract these high-symmetry points \n",
    "mc.featureExtract(mc.slice, sigma=5, fwhm=10, sigma_radius=3)\n",
    "mc.view(points=mc.features, annotated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate thin plate spline symmetry correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a non-linear coordinate transformation based on thin plate splines that restores 6-fold symmetry\n",
    "mc.splineWarpEstimate(image=mc.slice, landmarks=mc.pouter_ord, include_center=True,\n",
    "                      iterative=False, interp_order=2, update=True)\n",
    "mc.view(image=mc.slice_transformed, annotated=True, points={'feats':mc.ptargs}, backend='bokeh', crosshair=True, radii=[75,110,150], crosshair_thickness=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a coordinate translation to move the image into the center of the detector\n",
    "mc.coordinateTransform(type='translation', xtrans=70., ytrans=70., keep=True)\n",
    "plt.imshow(mc.slice_transformed, origin='lower', cmap='terrain_r')\n",
    "plt.axvline(x=256)\n",
    "plt.axhline(y=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the image into a high-symmetry direction\n",
    "mc.coordinateTransform( type='rotation', angle=-5, center=(256., 256.), keep=True)\n",
    "plt.imshow(mc.slice_transformed, origin='lower', cmap='terrain_r')\n",
    "plt.axvline(x=256)\n",
    "plt.axhline(y=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final deformation field\n",
    "subs = 20\n",
    "plt.scatter(mc.cdeform_field[::subs,::subs].ravel(), mc.rdeform_field[::subs,::subs].ravel(), c='b')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one high-symmetry point\n",
    "point_b = [252.,255.]\n",
    "# Pick the BZ center\n",
    "point_a = [308.,346.]\n",
    "# give the distance of the two in inverse Angstrom\n",
    "distance = np.pi*4/3/3.297\n",
    "# Momentum calibration assuming equal scaling along both x and y directions (equiscale=True)\n",
    "# Requirements : pixel coordinates of and the momentum space distance between two symmetry points, \n",
    "# plus the momentum coordinates\n",
    "# of one of the two points \n",
    "ext = mc.calibrate(mc.slice_transformed,\n",
    "                   point_from=point_a,\n",
    "                   point_to=point_b,\n",
    "                   dist=distance,\n",
    "                   equiscale=True,\n",
    "                   ret=['extent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display corrected image in calibrated coordinates\n",
    "mc.view(image=mc.slice_transformed, imkwds=ext)\n",
    "plt.xlabel('$k_x$', fontsize=15)\n",
    "plt.ylabel('$k_y$', fontsize=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin traces for energy calibration\n",
    "axes = ['t']\n",
    "bins = [1000]\n",
    "ranges = [(63000, 80000)]\n",
    "traces, tof = fp.extractEDC(folder=ECAL,\n",
    "                            axes=axes, bins=bins, ranges=ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied bias voltages (negated, in order to achieve negative binding energies, E-E_F)\n",
    "voltages = np.arange(-12.2, -23.2, -1)\n",
    "ec = aly.EnergyCalibrator(biases=voltages, traces=traces, tof=tof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize traces to maximum\n",
    "ec.normalize(smooth=True, span=7, order=1)\n",
    "ec.view(traces=ec.traces_normed, xaxis=ec.tof, backend='bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a TOF feature range, and translate it for each of the traces according to their shift in bias voltage\n",
    "rg = [(65000, 65200)]\n",
    "ec.addFeatures(traces=ec.traces_normed, refid=0, ranges=rg[0], infer_others=True, mode='append')\n",
    "ec.featranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first peak from each feature range\n",
    "ec.featureExtract(traces=ec.traces_normed, ranges=ec.featranges)\n",
    "ec.view(traces=ec.traces_normed, peaks=ec.peaks, backend='bokeh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate energy calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the energy calibration (3rd order polynom). Eref corresponds to the binding energy (E-E_F) of the selected feature in the refid trace.\n",
    "refid=5\n",
    "Eref=-1.3\n",
    "axs = ec.calibrate(ret='all', Eref=Eref, t=ec.tof, refid=refid)\n",
    "ec.view(traces=ec.traces_normed, xaxis=ec.calibration['axis'], backend='bokeh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the quality of the energy calibration\n",
    "for i in range(0,len(voltages)):\n",
    "    plt.plot(ec.calibration['axis']-(voltages[i]-voltages[refid]), ec.traces_normed[i])\n",
    "plt.xlim([-15,5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect calibration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy calibration function vs. TOF\n",
    "ec.view(traces=ec.calibration['axis'][None,:], xaxis=ec.tof, backend='matplotlib', show_legend=False)\n",
    "plt.scatter(ec.peaks[:,0], ec.biases-ec.biases[refid]+Eref, s=50, c='k')\n",
    "plt.xlabel('Time-of-flight', fontsize=15)\n",
    "plt.ylabel('Energy (eV)', fontsize=15)\n",
    "plt.ylim([-8,6])\n",
    "plt.xlim([63400,69800])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dask data frame processor\n",
    "dfp = fp.dataframeProcessor(datafolder=FDIR)\n",
    "dfp.read(source='folder', ftype='h5', timeStamps=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply energy calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the energy calibration\n",
    "dfp.appendEAxis(E0=ec.calibration['E0'], a=ec.calibration['coeffs'])\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply distortion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the distortion correction\n",
    "dfp.applyKCorrection(type='tps_matrix',\n",
    "                     rdeform_field = mc.rdeform_field,\n",
    "                     cdeform_field = mc.cdeform_field,\n",
    "                     X='X', Y='Y', newX='Xm', newY='Ym')\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply momentum calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the momentum calibration\n",
    "dfp.appendKAxis(point_b[0], point_b[1], X='Xm', Y='Ym', rstart=parp.binranges[0][0],\n",
    "                cstart=parp.binranges[1][0],\n",
    "                rstep=parp.binsteps[0],\n",
    "                cstep=parp.binsteps[1],\n",
    "                fc=mc.calibration['coeffs'][0],\n",
    "                fr=mc.calibration['coeffs'][1])\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply pump-probe delay axis conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pump-probe delay from the ADC coordinates\n",
    "ADCRange = (650, 6900)\n",
    "timeRange = (-100, 200)\n",
    "dfp.edf['delay'] = timeRange[0] + (dfp.edf['ADC']-ADCRange[0]) *\\\n",
    "    (timeRange[1] - timeRange[0])/(ADCRange[1]-ADCRange[0])\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin 4D data in transformed grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the 4-dimensional binning\n",
    "axes = ['kx', 'ky', 'E', 'delay']\n",
    "bins = [50, 50, 100, 21]\n",
    "ranges = [(-2, 2), (-2, 2), (-3, 2), (-110, 190)]\n",
    "# jittering of energy and ADC should best be done on the bin size of the hardware, \n",
    "# not the rebinned bin size. This requires reverse-calculating the jitter amplitudes\n",
    "# from the bin sizes.\n",
    "TOFrange=[64500,67000]\n",
    "e_t_conversion = (base.tof2evpoly(ec.calibration['coeffs'],\n",
    "                                  ec.calibration['E0'], \n",
    "                                  TOFrange[0])\n",
    "                  - base.tof2evpoly(ec.calibration['coeffs'],\n",
    "                                    ec.calibration['E0'], TOFrange[1])\n",
    "                 ) / (TOFrange[1] - TOFrange[0])\n",
    "d_adc_conversion = (timeRange[1] - timeRange[0]) / (ADCRange[1] - ADCRange[0])\n",
    "jitter_amplitude = [0.5,\n",
    "                    0.5,\n",
    "                    1*bins[2]/abs(ranges[2][1]-ranges[2][0])*e_t_conversion,\n",
    "                    1*bins[3]/abs(ranges[3][1]-ranges[3][0])*d_adc_conversion]\n",
    "dfp.distributedBinning(axes=axes,\n",
    "                       nbins=bins,\n",
    "                       ranges=ranges,\n",
    "                       scheduler='threads',\n",
    "                       ret=False,\n",
    "                       jittered=True,\n",
    "                       jitter_amplitude=jitter_amplitude)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell to store metadata from EPICS archive only if outside the FHI network\n",
    "This adds additional metadata to the xarray. This data may also be provided through additional ELN entries through a NOMAD instance or with a handwritten file directly to the mpes parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\"file\": {}}\n",
    "metadata['file'][\"KTOF:Lens:Extr:I\"] = -0.12877\n",
    "metadata['file'][\"KTOF:Lens:UDLD:V\"] = 399.99905\n",
    "metadata['file'][\"KTOF:Lens:Sample:V\"] = 17.19976\n",
    "metadata['file'][\"KTOF:Apertures:m1.RBV\"] = 3.729931\n",
    "metadata['file'][\"KTOF:Apertures:m2.RBV\"] = -5.200078\n",
    "metadata['file'][\"KTOF:Apertures:m3.RBV\"] = -11.000425\n",
    "\n",
    "# Sample motor positions\n",
    "metadata['file']['trARPES:Carving:TRX.RBV'] = 7.1900000000000004\n",
    "metadata['file']['trARPES:Carving:TRY.RBV'] = -6.1700200225439552\n",
    "metadata['file']['trARPES:Carving:TRZ.RBV'] = 33.4501953125\n",
    "metadata['file']['trARPES:Carving:THT.RBV'] = 423.30500940561586\n",
    "metadata['file']['trARPES:Carving:PHI.RBV'] = 0.99931647456264949\n",
    "metadata['file']['trARPES:Carving:OMG.RBV'] = 11.002500171914066"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate xarray\n",
    "Remember to remove the optional argument, metadata_dict, from the gather_metadata() function if the previous cell was not run.\n",
    "The missing archive metadata warnings are not critical for this example and can thus be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "res_xarray = dfp.gather_metadata(metadata_dict=copy.deepcopy(metadata), ec=ec, mc=mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a NeXus file from a xarray\n",
    "This conversion basically follows the same procedure as in the [convert to NeXus example](./E1%20Convert%20to%20Nexus.ipynb). Please refer to this notebook for details on the convert function. Here, we are using the objects keywords of `convert` to pass the generated xarray directly, instead of loading a h5 datafile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynxtools.dataconverter.convert import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(input_file=[\"config_file.json\", \"WSe2_eln.yaml\"],\n",
    "        objects=res_xarray,\n",
    "        reader='mpes',\n",
    "        nxdl='NXmpes',\n",
    "        output='WSe2.mpes.nxs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## View the data with H5Web\n",
    "H5Web is a tool for visualizing any data in the h5 data format. Since the NeXus format builds opon h5 it can be used to view this data as well. We just import the package and call H5Web with the output filename from the convert command above. For an analysis on NeXus data files please refer to [analysis example](./E3%20pyARPES%20analysis.ipynb).\n",
    "\n",
    "You can also view this data with the H5Viewer or other tools from your local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterlab_h5web import H5Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H5Web('WSe2.mpes.nxs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2ecc4d45d4efcd07af778d75fd26bf86d0642a6646ea5c57f06d5857684e419"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
