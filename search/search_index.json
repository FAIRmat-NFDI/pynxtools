{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"FAIRmat NeXus documentation","text":"<p>Within FAIRmat, we are extending the NeXus data format standard to support the FAIR data principles for experimental data in materials science (covering solid-state physics and the chemical physics of solids, as well as materials engineering). This is the documentation for both our contribution to the NeXus standard and for our tools for data conversion and verification.</p> <p><code>pynxtools</code>, the main tool under development, provides a data converter that maps experimental data and metadata to the NeXus format, performing parsing, normalization, visualization, and ontology matching. It combines various instrument output formats and electronic lab notebook (ELN) formats to an HDF5 file according to NeXus application definitions. In addition, <code>pynxtools</code> can be used to validate and verify NeXus files.</p> <p><code>pynxtools</code> offers scientists a convenient way to use the NeXus format and solves the challenge of unstructured and non-standardized data in experimental materials science. We consider this package useful for meeting the following FAIR principle as defined in FAIR Principles: Interpretations and Implementation Considerations: F2-4, I2-I3, and R1.</p> <p>FAIRmat's contribution to the existing NeXus standard, together with the tools provided through <code>pynxtools</code>, enable scientists and research groups working with data, as well as helping communities implement standardized FAIR research data.</p> <p>Additionally, the software is used as a plugin in the research data management system NOMAD for making experimental data searchable and publishable. NOMAD is developed by the FAIRMAT consortium, as a part of the German National Research Data Infrastructure (NFDI).</p> Project and community <p>The work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 460197019 (FAIRmat).</p>"},{"location":"index.html#tutorial","title":"Tutorial","text":"<p>A series of tutorials giving you an overview on how to store or convert your data to NeXus compliant files.</p> <ul> <li>Converting your data to NeXus</li> <li>Uploading NeXus data to NOMAD</li> </ul>"},{"location":"index.html#how-to-guides","title":"How-to guides","text":"<p>How-to guides provide step-by-step instructions for a wide range of tasks.</p> <ul> <li>Writing an application definition</li> <li>Storing data in multiple application definitions</li> <li>Build your own pynxtools plugin</li> <li>Implement a reader based on the MultiFormatReader</li> <li>Representing experimental geometries</li> <li>Using pynxtools test framework</li> </ul>"},{"location":"index.html#learn","title":"Learn","text":""},{"location":"index.html#an-introduction-to-nexus-and-its-design-principles","title":"An introduction to NeXus and its design principles","text":"<ul> <li>An introduction to NeXus</li> <li>Rules for storing data in NeXus</li> <li>The concept of multiple application definitions</li> </ul>"},{"location":"index.html#pynxtools","title":"pynxtools","text":"<ul> <li>Data conversion in <code>pynxtools</code></li> <li>Validation of NeXus files</li> <li>The MultiFormatReader as a reader superclass</li> </ul>"},{"location":"index.html#reference","title":"Reference","text":""},{"location":"index.html#nexus-definitions","title":"NeXus definitions","text":"<p>Here, you find the detailed list of application definitions and base classes and their respective fields.</p> <p>Or go directly to the official NIAC  or latest FAIRmat definitions.</p> <p>Note: To connect NeXus concepts with semantic web tools, efforts are underway to represent them using the W3C Web Ontology Language (OWL). See the NeXusOntology for more details.</p>"},{"location":"index.html#pynxtools_1","title":"pynxtools","text":"<p><code>pynxtools</code> has a number of command line tools that can be used to convert data and verify NeXus files. You can find more information about the API here.</p> <p>Within FAIRmat, we maintain a number of generic built-in pynxtools readers, together with reader plugins for different experimental techniques. Here you can find more information:</p> <ul> <li>Built-in pynxtools readers</li> <li>FAIRMat-supported pynxtools plugins</li> </ul>"},{"location":"how-tos/build-a-plugin.html","title":"Build your own pynxtools plugin","text":"<p>The pynxtools dataconverter is used to convert experimental data to NeXus/HDF5 files based on any provided NXDL schemas. The converter allows extending support to other data formats by allowing extensions called <code>readers</code>.  There exist a set of built-in pynxtools readers as well as pynxtools plugins to convert supported data files for some experimental techniques into compliant NeXus files.</p> <p>Your current data is not supported yet by the built-in pynxtools readers or the officially supported pynxtools plugins?</p> <p>Don't worry, the following how-to will guide you through the steps of writing a reader for your own data.</p>"},{"location":"how-tos/build-a-plugin.html#getting-started","title":"Getting started","text":"<p>You should start by creating a clean repository that implements the following structure (for a plugin called <code>pynxtools-plugin</code>): <pre><code>pynxtools-plugin\n\u251c\u2500\u2500 .github/workflows\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 explanation\n\u2502   \u251c\u2500\u2500 how-tos\n\u2502   \u251c\u2500\u2500 reference\n\u2502   \u251c\u2500\u2500 tutorial\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 pynxtools_plugin\n\u2502       \u251c\u2500\u2500 reader.py\n\u251c\u2500\u2500 tests\n\u2502   \u2514\u2500\u2500 data\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 mkdocs.yaml\n\u251c\u2500\u2500 dev-requirements.txt\n\u2514\u2500\u2500 pyproject.toml\n</code></pre></p> <p>To identify <code>pynxtools-plugin</code> as a plugin for pynxtools, an entry point must be established (in the <code>pyproject.toml</code> file): <pre><code>[project.entry-points.\"pynxtools.reader\"]\nmydatareader = \"pynxtools_plugin.reader:MyDataReader\"\n</code></pre></p> <p>Note that it is also possible that your plugin contains multiple readers. In that case, each reader must have its unique entry point.</p> <p>Here, we will focus mostly on the <code>reader.py</code> file and how to build a reader. For guidelines on how to build the other parts of your plugin, you can have a look here:</p> <ul> <li>Documentation writing guide</li> <li>Plugin testing framework</li> </ul>"},{"location":"how-tos/build-a-plugin.html#writing-a-reader","title":"Writing a Reader","text":"<p>After you have established the main structure, you can start writing your reader. The new reader shall be placed in <code>reader.py</code>.</p> <p>Then implement the reader function:</p> reader.py<pre><code>\"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\nfrom typing import Tuple, Any\n\nfrom pynxtools.dataconverter.readers.base.reader import BaseReader\n\nclass MyDataReader(BaseReader):\n    \"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\n\n    supported_nxdls = [\n        \"NXmynxdl\" # this needs to be changed during implementation.\n    ]\n\n    def read(\n        self,\n        template: dict = None,\n        file_paths: Tuple[str] = None,\n        objects: Tuple[Any] = None\n    ) -&gt; dict:\n        \"\"\"Reads data from given file and returns a filled template dictionary\"\"\"\n        # Here, you must provide functionality to fill the the template, see below.\n        # Example:\n        # template[\"/entry/instrument/name\"] = \"my_instrument\"\n\n        return template\n\n\n# This has to be set to allow the convert script to use this reader. Set it to \"MyDataReader\".\nREADER = MyDataReader\n</code></pre>"},{"location":"how-tos/build-a-plugin.html#the-reader-template-dictionary","title":"The reader template dictionary","text":"<p>The read function takes a <code>Template</code> dictionary, which is used to map from the measurement (meta)data to the concepts defined in the NeXus application definition. The template contains keys that match the concepts in the provided NXDL file.</p> <p>The returned template dictionary should contain keys that exist in the template as defined below. The values of these keys have to be data objects to populate the output NeXus file. They can be lists, numpy arrays, numpy bytes, numpy floats, numpy ints, ... . Practically you can pass any value that can be handled by the <code>h5py</code> package.</p> <p>Example for a template entry:</p> <pre><code>{\n  \"/entry/instrument/source/type\": \"None\"\n}\n</code></pre> <p>For a given NXDL schema, you can generate an empty template with the command <pre><code>user@box:~$ dataconverter generate-template --nxdl NXmynxdl\n</code></pre></p>"},{"location":"how-tos/build-a-plugin.html#naming-of-groups","title":"Naming of groups","text":"<p>In case the NXDL does not define a <code>name</code> for the group the requested data belongs to, the template dictionary will list it as <code>/NAME_IN_NXDL[name_in_output_nexus]</code>. You can choose any name you prefer instead of the suggested <code>name_in_output_nexus</code> (see here for the naming conventions). This allows the reader function to repeat groups defined in the NXDL to be outputted to the NeXus file.</p> <pre><code>{\n  \"/ENTRY[my_entry]/INSTRUMENT[my_instrument]/SOURCE[my_source]/type\": \"None\"\n}\n</code></pre>"},{"location":"how-tos/build-a-plugin.html#attributes","title":"Attributes","text":"<p>For attributes defined in the NXDL, the reader template dictionary will have the assosciated key with a \"@\" prefix to the attributes name at the end of the path:</p> <pre><code>{\n  \"/entry/instrument/source/@attribute\": \"None\"\n}\n</code></pre>"},{"location":"how-tos/build-a-plugin.html#units","title":"Units","text":"<p>If there is a field defined in the NXDL, the converter expects a filled in /data/@units entry in the template dictionary corresponding to the right /data field unless it is specified as NX_UNITLESS in the NXDL. Otherwise, a warning will be shown.</p> <pre><code>{\n  \"/ENTRY[my_entry]/INSTRUMENT[my_instrument]/SOURCE[my_source]/data\": \"None\",\n  \"/ENTRY[my_entry]/INSTRUMENT[my_instrument]/SOURCE[my_source]/data/@units\": \"Should be set to a string value\"\n}\n</code></pre>"},{"location":"how-tos/build-a-plugin.html#links","title":"Links","text":"<p>You can also define links by setting the value to sub dictionary object with key <code>link</code>:</p> <pre><code>template[\"/entry/instrument/source\"] = {\"link\": \"/path/to/source/data\"}\n</code></pre>"},{"location":"how-tos/build-a-plugin.html#building-off-of-the-basereader","title":"Building off of the BaseReader","text":"<p>When building off the <code>BaseReader</code>, the developer has the most flexibility. Any new reader must implement the <code>read</code> function, which must return a filled template object.</p>"},{"location":"how-tos/build-a-plugin.html#building-off-of-the-multiformatreader","title":"Building off of the MultiFormatReader","text":"<p>While building on the <code>BaseReader</code> allows for the most flexibility, in most cases it is desirable to implement a reader that can read in multiple file formats and then populate the template based on the read data. For this purpose, <code>pynxtools</code> has the <code>MultiFormatReader</code>, which can be readily extended for your own data.</p> <p>You can find an extensive how-to guide to build off the <code>MultiFormatReader</code> here.</p>"},{"location":"how-tos/build-a-plugin.html#calling-the-reader-from-the-command-line","title":"Calling the reader from the command line","text":"<p>The dataconverter can be executed using: <pre><code>user@box:~$ dataconverter --reader mydatareader --nxdl NXmynxdl --output path_to_output.nxs\n</code></pre> Here, the <code>--reader</code> flag must match the reader name defined in <code>[project.entry-points.\"pynxtools.reader\"]</code> in the pyproject.toml file. The NXDL name passed to <code>--nxdl</code>must be a valid NeXus NXDL/XML file in <code>pynxtools.definitions</code>.</p> <p>Aside from this default structure, there are many more flags that can be passed to the dataconverter call. Here is its API:</p>"},{"location":"how-tos/build-a-plugin.html#dataconverter","title":"dataconverter","text":"<p>This command allows you to use the converter functionality of the dataconverter.</p> <p>Usage:</p> <pre><code>dataconverter [OPTIONS] [FILES]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--input-file</code> text Deprecated: Please use the positional file arguments instead. The path to the input data file to read. Repeat for more than one file. default=[] This option is required if no '--params-file' is supplied. <code>[]</code> <code>--reader</code> choice (<code>example</code> | <code>json_map</code> | <code>json_yml</code> | <code>multi</code>) The reader to use. Examples are json_map or readers from a pynxtools plugin. default='json_map' This option is required if no '--params-file' is supplied. <code>json_map</code> <code>--nxdl</code> text The name of the NeXus application definition file to use without the extension nxdl.xml. This option is required if no '--params-file' is supplied. None <code>--output</code> text The path to the output NeXus file to be generated. default='output.nxs' <code>output.nxs</code> <code>--params-file</code> filename Allows to pass a .yaml file with all the parameters the converter supports. None <code>--ignore-undocumented</code> boolean Ignore all undocumented fields during validation. <code>False</code> <code>--fail</code> boolean Fail conversion and don't create an output file if the validation fails. <code>False</code> <code>--skip-verify</code> boolean Skips the verification routine during conversion. <code>False</code> <code>--mapping</code> text Takes a .mapping.json file and converts data from given input files. None <code>-c</code>, <code>--config</code> file A json config file for the reader None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"how-tos/transformations.html","title":"Storing experimental geometries","text":"<p>Work in progress</p> <p>This part of the documentation is still being written and it might be confusing or incomplete.</p>"},{"location":"how-tos/use-multi-format-reader.html","title":"How to use the built-in MultiFormatReader","text":"<p>While building on the <code>BaseReader</code> allows for the most flexibility, in most cases it is desirable to implement a reader that can read in multiple file formats and then populate the template based on the read data. For this purpose, <code>pynxtools</code> has the <code>MultiFormatReader</code>, which can be readily extended for your own data. In this how-to guide, we will focus on an implementation using a concrete example. If you are also interested in the general structure of the <code>MultiFormatReader</code>, you can find more information here.</p>"},{"location":"how-tos/use-multi-format-reader.html#getting-started","title":"Getting started","text":"<p>Note: You can find all of the data and the developed python scripts here.</p> <p>Here, we will implement a reader called <code>MyDataReader</code> that builds on the <code>MultiFormatReader</code>. <code>MyDataReader</code> is an example for a reader that can read HDF5 data from a specific technology-partner data set, as well as additional metadata from am electronic lab notebook (in YAML format).</p> <p>For demonstration purposess, we will work with a very simple mock application definition:</p> NXsimple.nxdl.xml<pre><code>&lt;definition xmlns=\"http://definition.nexusformat.org/nxdl/3.1\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" category=\"application\" type=\"group\" name=\"NXsimple\" extends=\"NXobject\" xsi:schemaLocation=\"http://definition.nexusformat.org/nxdl/3.1 ../nxdl.xsd\"&gt;\n    &lt;doc&gt;\n         Mock application definition.\n    &lt;/doc&gt;\n    &lt;group type=\"NXentry\"&gt;\n        &lt;field name=\"definition\"&gt;\n            &lt;attribute name=\"version\"/&gt;\n            &lt;enumeration&gt;\n                &lt;item value=\"NXsimple\"/&gt;\n            &lt;/enumeration&gt;\n        &lt;/field&gt;\n        &lt;field name=\"title\"/&gt;\n        &lt;group type=\"NXuser\" recommended=\"true\"&gt;\n            &lt;field name=\"name\"&gt;\n                &lt;doc&gt;\n                     Name of the user.\n                &lt;/doc&gt;\n            &lt;/field&gt;\n            &lt;field name=\"address\" recommended=\"true\"&gt;\n                &lt;doc&gt;\n                     Name of the affiliation of the user.\n                &lt;/doc&gt;\n            &lt;/field&gt;\n        &lt;/group&gt;\n        &lt;group type=\"NXinstrument\"&gt;\n            &lt;doc&gt;\n                 Description of the instrument and its individual parts.\n            &lt;/doc&gt;\n            &lt;attribute name=\"version\"&gt;\n                &lt;doc&gt;\n                     Version of the instrument.\n                &lt;/doc&gt;\n            &lt;/attribute&gt;\n            &lt;group type=\"NXdetector\"&gt;\n                &lt;field name=\"count_time\" type=\"NX_NUMBER\" units=\"NX_TIME\" recommended=\"true\"&gt;\n                    &lt;doc&gt;\n                         Elapsed actual counting time\n                    &lt;/doc&gt;\n                &lt;/field&gt;\n            &lt;/group&gt;\n        &lt;/group&gt;\n        &lt;group name=\"sample\" type=\"NXsample\"&gt;\n            &lt;field name=\"name\"/&gt;\n            &lt;field name=\"physical_form\" recommended=\"true\"/&gt;\n            &lt;field name=\"temperature\" type=\"NX_FLOAT\" recommended=\"true\" units=\"NX_TEMPERATURE\"/&gt;\n        &lt;/group&gt;\n        &lt;group name=\"data\" type=\"NXdata\"&gt;\n            &lt;doc&gt;\n                 The default NXdata group containing a view on the measured data.\n            &lt;/doc&gt;\n        &lt;/group&gt;\n    &lt;/group&gt;\n&lt;/definition&gt;\n</code></pre> <p>The NXDL requires a user, some sample information, some instrument metadata, and the measured data to be written. Some groups, fields, and attributes are strictly required (the default), others just recommended.</p> <p>Note that in order to be recognized as a valid application definition, this file should be copied to the <code>definitions</code> submodule at <code>pynxtools.definitions</code>.</p> <p>We first start by implementing the class and its <code>__init__</code> call: reader.py<pre><code>\"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\nfrom typing import Tuple, Any\n\nfrom pynxtools.dataconverter.readers.base.reader import ParseJsonCallbacks, MultiFormatReader\n\nclass MyDataReader(MultiFormatReader):\n    \"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\n\n    supported_nxdls = [\n        \"NXsimple\"\n    ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.extensions = {\n            \".yml\": self.handle_eln_file,\n            \".yaml\": self.handle_eln_file,\n            \".json\": self.set_config_file,\n            \".hdf5\": self.handle_eln_file,\n            \".h5\": self.handle_eln_file,\n        }\n\nREADER = MyDataReader\n</code></pre> Note that here we are adding handlers for three types of data file extensions: 1. <code>\".hdf5\"</code>, <code>\".h5\"</code>: This will be used to parse in the (meta)data from the instrument's HDF5 file. 2. <code>\".yml\"</code>, <code>\".yaml\"</code>: This will be used to parse in the (meta)data from the ELN file. 3. <code>\".json\"</code>: This will be used to read in the config file, which is used to map from the (meta)data concepts from the instrument and ELN data to the concepts in the NXDL file.</p>"},{"location":"how-tos/use-multi-format-reader.html#reading-in-the-instruments-data-and-metadata","title":"Reading in the instrument's data and metadata","text":"<p>First, we will have a look at the HDF5 file. This mock HDF5 file was generated with <code>h5py</code> using a simple script.</p> <p></p> <p>Here, we see that we have a <code>data</code> group with x and y values, as well as some additional metadata for the instrument.</p> <p>Here is one way to implement the method to read in the data: reader.py<pre><code>import h5py\n\ndef handle_hdf5_file(filepath):\n    def recursively_read_group(group, path=\"\"):\n        result = {}\n        for key, item in group.items():\n            new_path = f\"{path}/{key}\" if path else key\n            if isinstance(item, h5py.Group):\n                # Recursively read subgroups\n                result.update(recursively_read_group(item, new_path))\n            elif isinstance(item, h5py.Dataset):\n                # Read datasets\n                result[new_path] = item[()]\n        return result\n\n    # Open the HDF5 file and read its contents\n    with h5py.File(filepath, \"r\") as hdf:\n        self.hdf5_data = recursively_read_group(hdf)\n\n    return {}\n</code></pre> Note that here we are returning an empty dictionary because we don't want to fill the template just yet, but only read in the HDF5 data for now. We will use the config file later to fill the template with the read-in data. Note that it is also possible to return a dictionary here to update the template directly.</p> <p><code>self.hdf5_data</code> will look like this: <pre><code>{\n    \"data/x_values\": array([-10.        ,  -9.7979798 ,  -9.5959596 , ...,  10.        ]),\n    \"data/y_values\": array([3.72665317e-06, 6.14389891e-06, 1.00262383e-05, ..., 3.72665317e-06]),\n    \"data/x_units\": \"eV\",\n    \"data/y_units\": \"counts_per_second\",\n    \"metadata/instrument/version\": 1.0,\n    \"metadata/instrument/detector/name\": \"my_gaussian_detector\",\n    \"metadata/instrument/detector/count_time\": 1.2,\n    \"metadata/instrument/detector/count_time_units\": s\",\n}\n</code></pre></p>"},{"location":"how-tos/use-multi-format-reader.html#reading-in-eln-data","title":"Reading in ELN data","text":"<p>As we can see in the application definition <code>NXsimple</code> above, there are some concepts defined for which there is no equivalent metadata in the HDF5 file. We are therefore using a YAML ELN file to add additional metadata. The ELN file <code>eln_data.yaml</code> looks like this: eln_data.yaml<pre><code>title: My experiment\nuser:\n  name: John Doe\n  address: 123 Science Rd, Data City, DC\nsample:\n  name: my_sample\n  physical_form: powder\n  temperature:\n    value: 300\n    unit: K\n</code></pre></p> <p>It contains metadata about the user and the sample that was measured.</p> <p>We now need to write a function to read in this ELN data. Luckily, there exists already a solution within <code>pynxtools</code>, using the <code>parse_yaml</code> function:</p> <p>reader.py<pre><code>from pynxtools.dataconverter.readers.utils import parse_yml\n\nCONVERT_DICT = {\n    \"unit\": \"@units\",\n    \"version\": \"@version\",\n    \"user\": \"USER[user]\",\n    \"instrument\": \"INSTRUMENT[instrument]\",\n    \"detector\": \"DETECTOR[detector]\",\n    \"sample\": \"SAMPLE[sample]\",\n}\n\ndef handle_eln_file(self, file_path: str) -&gt; Dict[str, Any]:\n    self.eln_data = parse_yml(\n        file_path,\n        convert_dict=CONVERT_DICT,\n        parent_key=\"/ENTRY[entry]\",\n    )\n\n    return {}\n</code></pre> When this method is called, <code>self.eln_data</code> will look like this: <pre><code>{\n    \"/ENTRY[entry]/title\": \"My experiment\",\n    \"/ENTRY[entry]/USER[user]/name\": \"John Doe\",\n    \"/ENTRY[entry]/USER[user]/address\": \"123 Science Rd, Data City, DC\",\n    \"/ENTRY[entry]/SAMPLE[sample]/name\": \"my_sample\",\n    \"/ENTRY[entry]/SAMPLE[sample]/physical_form\": \"powder\",\n    \"/ENTRY[entry]/SAMPLE[sample]/temperature\": 300,\n    \"/ENTRY[entry]/SAMPLE[sample]/temperature/@units\": \"K\"\n}\n</code></pre> Note that here we are using <code>parent_key=\"/ENTRY[entry]\"</code> as well as a <code>CONVERT_DICT</code>, meaning that each key in <code>self.eln_data</code> will start with <code>\"/ENTRY[entry]\"</code> and some of the paths will be converted to match the template notation. This will be important later.</p>"},{"location":"how-tos/use-multi-format-reader.html#parsing-the-config-file","title":"Parsing the config file","text":"<p>Next up, we can make use of the config file, which is a JSON file that tells the reader how to map the concepts from the HDF5 and ELN files in order to populate the template designed to match <code>NXsimple</code>. The choices made in the config file define how semantics from the source (data file) and target (NeXus application definition) side are mapped. Essentially, the config file should contain all keys that are present in the NXDL. In our case, the config file looks like this:</p> <p>config_file.json<pre><code>{\n  \"/ENTRY/title\": \"@eln\", \n  \"/ENTRY/USER[user]\": {\n    \"name\":\"@eln\",\n    \"address\":@eln:\"/ENTRY/USER[user]/address\",\n  }, \n  \"/ENTRY/INSTRUMENT[instrument]\": {\n    \"@version\":\"@attrs:metadata/instrument/version\",\n    \"DETECTOR[detector]\":{\n      \"count_time\":\"@attrs:metadata/instrument/detector/count_time\",\n      \"count_time/@units\":\"@attrs:metadata/instrument/detector/count_time_units\"\n    }\n  },\n  \"/ENTRY/SAMPLE[sample]\": {\n    \"name\":\"@eln\",\n    \"physical_form\":\"@eln\",\n    \"temperature\":\"@eln\",\n    \"temperature/@units\":\"@eln\"\n  },\n  \"/ENTRY/data\": {\n    \"@axes\":[\"x_values\"],\n    \"@signal\": \"data\",\n    \"data\": \"@data:y_values\",\n    \"data/@units\": \"@attrs:data/y_units\",   \n    \"x_values/@units\": \"@attrs:data/x_units\",\n    \"x_values/@units\": \"@data:x_values\"\n  }\n}\n</code></pre> Note that here we are using <code>@</code>-prefixes which are used to fill the template from the different data sources. We dicuss this below in more detail.</p> <p>We also implement a method for setting the config file in the reader: reader.py<pre><code>def set_config_file(self, file_path: str) -&gt; Dict[str, Any]:\n    if self.config_file is not None:\n        logger.info(\n            f\"Config file already set. Replaced by the new file {file_path}.\"\n        )\n    self.config_file = file_path\n\n    return {}\n</code></pre></p>"},{"location":"how-tos/use-multi-format-reader.html#filling-the-template-from-the-read-in-data","title":"Filling the template from the read-in data","text":"<p>Finally, after reading in all of the data and metadata as well as designing the config file, we can start filling the template. For this, we must implement functions that are called using the reader's callbacks.</p> <p>We will start with the <code>@attrs</code> prefix, associated with the <code>attrs_callback</code>. We must implement the <code>get_attr</code> method: reader.py<pre><code>def get_attr(self, key: str, path: str) -&gt; Any:\n    \"\"\"\n    Get the metadata that was stored in the main file.\n    \"\"\"\n    if self.hdf5_data is None:\n        return None\n\n    return self.hdf5_data.get(path)\n</code></pre> This method (and all similar callbacks methods) have two inputs: 1. <code>key</code>, which is a key in the config file. Note that here, the generic <code>\"/ENTRY/\"</code> gets replaced by <code>f\"/ENTRY[{entry_name}]/\"</code>, where <code>entry_name</code> is the one of the entries of the <code>self.get_entry_names</code> method. 2. <code>path</code>, which is the part of the config value that comes after the <code>@attrs:</code> prefix. For example, for the config value <code>\"@attrs:my-metadata\"</code>, the extracted path is <code>my-metadata</code>.</p> <p>For the <code>get_attr</code> method, we are making use of the <code>path</code>. For example, for the config value <code>\"@attrs:metadata/instrument/version\"</code>, the extracted path is <code>metadata/instrument/version</code>, which is also one of the keys of the <code>self.hdf5_data</code> dictionary.</p> <p>For the ELN data, we must implement the <code>get_eln_data</code> function that gets called from the <code>eln_callback</code> when using the <code>@eln</code> prefix: reader.py<pre><code>def get_eln_data(self, key: str, path: str) -&gt; Any:\n        \"\"\"Returns data from the given eln path.\"\"\"\n        if self.eln_data is None:\n            return None\n\n        return self.eln_data.get(key)\n</code></pre> Here, we are making use of the fact that we have used <code>CONVERT_DICT</code> in the <code>parse_yml</code> function above. Thus, the keys of the <code>self.eln_data</code> dictionary are exactly the same as those in the config file (for example, the config key <code>\"/ENTRY[entry]/USER[user]/address\"</code> also exists in <code>self.eln_data</code>). Therefore, we can just get this data using the <code>key</code> coming from the config file. </p> <p>Finally, we also need to address the <code>@data</code> prefix, which gets used in the <code>data_callback</code> to populate the NXdata group in the template. Note that here we use the same <code>@data</code> prefix to fill the <code>x_values</code> as well as the <code>data</code> (from <code>y_values</code>) fields. We achieve this by using the path that follows <code>@data:</code> in the config file: reader.py<pre><code>def get_data(self, key: str, path: str) -&gt; Any:\n    \"\"\"Returns measurement data from the given hdf5 path.\"\"\"\n    if path.endswith((\"x_values\", \"y_values\")):\n        return self.hdf5_data.get(f\"data/{path}\")\n    else:\n        logger.warning(f\"No axis name corresponding to the path {path}.\")\n</code></pre></p>"},{"location":"how-tos/use-multi-format-reader.html#bringing-it-all-together","title":"Bringing it all together","text":"<p>Et voil\u00e0! That's all we need to read in our data and populate the <code>NXsimple</code> template. Our final reader looks like this:</p> reader.py<pre><code>import logging\nfrom typing import Dict, Any\nimport h5py\n\nfrom pynxtools.dataconverter.readers.multi.reader import MultiFormatReader\nfrom pynxtools.dataconverter.readers.utils import parse_yml\n\nlogger = logging.getLogger(\"pynxtools\")\n\nCONVERT_DICT = {\n    \"unit\": \"@units\",\n    \"version\": \"@version\",\n    \"user\": \"USER[user]\",\n    \"instrument\": \"INSTRUMENT[instrument]\",\n    \"detector\": \"DETECTOR[detector]\",\n    \"sample\": \"SAMPLE[sample]\",\n}\n\n\nclass MyDataReader(MultiFormatReader):\n    \"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\n\n    supported_nxdls = [\n        \"NXsimple\"\n    ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.extensions = {\n            \".yml\": self.handle_eln_file,\n            \".yaml\": self.handle_eln_file,\n           \".json\": self.set_config_file,\n            \".hdf5\": self.handle_hdf5_file,\n            \".h5\": self.handle_hdf5_file,\n        }\n\n    def set_config_file(self, file_path: str) -&gt; Dict[str, Any]:\n        if self.config_file is not None:\n            logger.info(\n                f\"Config file already set. Replaced by the new file {file_path}.\"\n            )\n        self.config_file = file_path\n        return {}\n\n    def handle_hdf5_file(self, filepath) -&gt; Dict[str, Any]:\n        def recursively_read_group(group, path=\"\"):\n            result = {}\n            for key, item in group.items():\n                new_path = f\"{path}/{key}\" if path else key\n                if isinstance(item, h5py.Group):\n                    # Recursively read subgroups\n                    result.update(recursively_read_group(item, new_path))\n                elif isinstance(item, h5py.Dataset):\n                    # Read datasets\n                    result[new_path] = item[()]\n            return result\n\n        # Open the HDF5 file and read its contents\n        with h5py.File(filepath, \"r\") as hdf:\n            self.hdf5_data = recursively_read_group(hdf)\n\n        return {}\n\n    def handle_eln_file(self, file_path: str) -&gt; Dict[str, Any]:\n        self.eln_data = parse_yml(\n            file_path,\n            convert_dict=CONVERT_DICT,\n            parent_key=\"/ENTRY[entry]\",\n        )\n\n        return {}\n\n    def get_attr(self, key: str, path: str) -&gt; Any:\n        \"\"\"\n        Get the metadata that was stored in the main file.\n        \"\"\"\n        if self.hdf5_data is None:\n            return None\n\n        return self.hdf5_data.get(path)\n\n    def get_eln_data(self, key: str, path: str) -&gt; Any:\n        \"\"\"Returns data from the given eln path.\"\"\"\n        if self.eln_data is None:\n            return None\n\n        return self.eln_data.get(key)\n\n    def get_data(self, key: str, path: str) -&gt; Any:\n        \"\"\"Returns measurement data from the given hdf5 path.\"\"\"\n        if path.endswith((\"x_values\", \"y_values\")):\n            return self.hdf5_data.get(f\"data/{path}\")\n        else:\n            logger.warning(f\"No axis name corresponding to the path {path}.\")  \n\nREADER = MyDataReader\n</code></pre>"},{"location":"how-tos/use-multi-format-reader.html#using-the-reader","title":"Using the reader","text":"<p>We can call our reader using the following command</p> <pre><code>user@box:~$ dataconverter mock_data.h5 eln_data.yaml -c config_file --reader mydatareader --nxdl NXsimple  --output output.nxs\n</code></pre> <p>The final <code>output.nxs</code> file gets automatically validated against <code>NXsimple</code>, so we can be sure that it is compliant with that application definition. Here is a look at our final NeXus file:</p> <p></p>"},{"location":"how-tos/using-multiple-appdefs.html","title":"Storing data following multiple appdefs","text":"<p>Work in progress</p> <p>This part of the documentation is still being written and it might be confusing or incomplete.</p>"},{"location":"how-tos/using-pynxtools-test-framework.html","title":"Generalized Test Functionality for <code>pynxtools</code> plugins","text":"<p>The <code>pynxtools</code> sub-package <code>testing</code> is to be utilized to write automated tests for pynxtools reader plugins without requiring in-depth knowledge of the pynxtools internal architecture. The tool supports generalised a general test for all reader plugins, irrespective of the technical details of the raw data files and the internal design of the plugin (note: it is assumed that the plugin was built from the plugin template or has the same structure internally).</p>"},{"location":"how-tos/using-pynxtools-test-framework.html#why-it-is-needed","title":"Why it is needed","text":"<p>To test integration of a plugin with the <code>pynxtools</code> core system, we need to: 1. Test the plugin's integration with <code>pynxtools</code> from the plugin's CI/CD. 2. Test in the pynxtools's CI/CD if the plugin has been integrated with <code>pynxtools</code> properly.</p>"},{"location":"how-tos/using-pynxtools-test-framework.html#how-to-write-an-integration-test-for-a-reader-plugin-with-pynxtoolstesting","title":"How to write an integration test for a reader plugin with <code>pynxtools.testing</code>","text":"<p>It is very simple to write a test to verify the plugin integration with <code>pynxtools</code> within the plugin's tests directory. The developer can place the test where they want, but they need to use the provided test interface from <code>pynxtools</code>. An example test for <code>pynxtools-FOO</code> (a demo plugin) plugin is given below:</p> <pre><code># test_plugin.py\n\nimport os\n\nimport pytest\nfrom pynxtools.testing.nexus_conversion import ReaderTest\n\n# e.g. module_dir = /pynxtools-foo/tests\nmodule_dir = os.path.dirname(os.path.abspath(__file__))\n\n\n@pytest.mark.parametrize(\n    \"nxdl,reader_name,files_or_dir\",\n    [\n        (\"NXfoo\", \"foo\", f\"{module_dir}/../tests/data/test_data_dir_1\"),\n        (\"NXfoo\", \"foo\", f\"{module_dir}/../tests/data/test_data_dir_2\")\n    ],\n)\ndef test_foo_reader(nxdl, reader_name, files_or_dir, tmp_path, caplog):\n    \"\"\"Test for the FooReader or foo reader plugin.\n\n    Parameters\n    ----------\n    nxdl : str\n        Name of the NXDL application definition that is to be tested by\n        this reader plugin (e.g. NXfoo), without the file ending .nxdl.xml.\n    reader_name : str\n        Name of the class of the reader (e.g. \"foo\")\n    files_or_dir : class\n        Name of the class of the reader.\n    tmp_path : pytest.fixture\n        Pytest fixture variable, used to create temporary file and clean up the generated files\n        after test.\n    caplog : pytest.fixture\n        Pytest fixture variable, used to capture the log messages during the test.\n    \"\"\"\n    # test plugin reader\n    test = ReaderTest(nxdl, reader_name, files_or_dir, tmp_path, caplog)\n    test.convert_to_nexus()\n    # test.convert_to_nexus(caplog_level=\"ERROR\", ignore_undocumented=True)\n    # Use `ignore_undocumented` to skip undocumented fields\n    # caplog_level can be \"ERROR\" or \"WARNING\"\n    test.check_reproducibility_of_nexus()\n</code></pre> <p>Alongside the test data in <code>test/data</code>, it is also possible to add other types of test data inside the test directory of the plugin.</p> <p>You can also pass additional parameters to <code>test.convert_to_nexus</code>:</p> <ul> <li> <p><code>caplog_level</code> (str): Can be either \"ERROR\" (by default) or \"warning\". This parameter determines the level at which the caplog is set during testing. If it is \"WARNING\", the test will also fail if any warnings are reported by the reader.</p> </li> <li> <p><code>ignore_undocumented</code> (boolean): If true, the test skipts the verification of undocumented keys. Otherwise, a warning massages for undocumented keys is raised</p> </li> </ul>"},{"location":"how-tos/writing-an-appdef.html","title":"Writing a Simple Application Definition","text":"<p>Work in progress</p> <p>This part of the documentation is still being written and it might be confusing or incomplete.</p> <p>Concept of this How-to:</p> <p>Create an example file NXdouble_slit</p> <p>NXslit_experiments --&gt; NXdouble_slit NXslit_experiments --&gt; NXsingle_slit</p> <p>They should learn the basic principles of how nexus works, the different path notations - Principles of nexus     - concepts     - appdefs     - base classes - Inheritance of application definitions and base classes - Connection of concept paths and instance paths - Description of appdef/base class notation (upper and lower case) - Basic tools for creation (pynxtools) and verification (pynxtools?) of nexus files</p> <p>Additional information (i.e., not in this tutorial but linked to this): - Creating a reader in pynxtools - Reading/writing nexus data in nomad</p>"},{"location":"learn/dataconverter-and-readers.html","title":"Data conversion in pynxtools","text":"<p>One of the main motivations for pynxtools is to develop a tool for combining various instrument output formats and electronic lab notebook (ELN) into a file according to NeXus application definitions. </p> <p>The <code>dataconverter</code> API in pynxtools provides exactly that: it converts experimental as well as simulation data, together with the results from analysis of such data, to NeXus files based on any provided NXDL schemas. Here, we are using HDF5 as the serialization format.</p> <p>The dataconverter currently has essentially three functionalities:</p> <ol> <li>Read in experimental data using <code>readers</code></li> <li>Validate the data and metadata against a NeXus application definition of choice (i.e., check that the output data matches all existence, shape, and format constraints of application definition)</li> <li>Write a valid NeXus/HDF5 file</li> </ol> <p>A set of readers has been developed which the converter calls to read in a set of experiment/method-specific file(s) and for a specific set of application definitions (NXDL XML file). These data files can be in a proprietary format, or of a certain format used in the respective scientific community, or text files. Only in combination, these files hold all the required pieces of information which the application definition demands and which are thus required to make a NeXus/HDF5 file compliant. Users can store additional pieces of information in an NeXus/HDF5 file. In this case readers will issue a warning that these data are not properly documented from the perspective of NeXus.</p> <p>There exists two different subsets of readers:</p> <ol> <li>Built-in readers, which are implemented directly in pynxtools and are typically used either as superclasses for new reader implementations or for generic reading purposes not directly related to any specific technique.</li> <li>Reader plugins for `pynxtools, which are used for reading data of specific experimental techniques and are typically available as their own Python packages.</li> </ol>"},{"location":"learn/dataconverter-and-readers.html#matching-to-nexus-application-definitions","title":"Matching to NeXus application definitions","text":"<p>The purpose of the dataconverter is to create NeXus/HDF5 files with content that matches a specific NeXus application definition. Such application definitions are useful for collecting a set of pieces of information about a specific experiment in a given scientific field. The pieces of information are numerical and categorical (meta)data. The application definition is used to provide these data in a format that serves a data delivery contract: The HDF5 file, or so-called NeXus file, delivers all those pieces of information which the application definition specifies. Required and optional pieces of information are distinguished. NeXus classes can recommend the inclusion of certain pieces of information. Recommended data are essentially optional. The idea is that flagging these data as recommended motivates users to collect these, but does not require to write dummy or nonsense data if the recommended data is not available.</p>"},{"location":"learn/dataconverter-and-readers.html#getting-started","title":"Getting started","text":"<p>Each of the built-in readers comes with the main <code>pynxtools</code> package. Hence, they can be used after after pip installation: <pre><code>user@box:~$ pip install pynxtools\n</code></pre></p> <p>The different FAIRmat-supported plugins can be installed together with pynxtools by passing the name of the plugin as an extra to the pip install call. For example, for the <code>pynxtools-mpes</code> plugin: <pre><code>pip install pynxtools[mpes]\n</code></pre></p> <p>In addition, it is also possible to install all of the pynxtools reader plugins which are maintained by FAIRmat by passing the <code>[convert]</code> extra to the pip install call:</p> <pre><code>pip install pynxtools[convert]\n</code></pre> <p>Note that in this case, the latest version of the plugin from PyPI is installed.</p>"},{"location":"learn/dataconverter-and-readers.html#usage","title":"Usage","text":"<p>See here for the documentation of the <code>dataconverter</code> API.</p>"},{"location":"learn/dataconverter-and-readers.html#use-with-multiple-input-files","title":"Use with multiple input files","text":"<pre><code>user@box:~$ dataconverter metadata data.raw otherfile --nxdl nxdl --reader &lt;reader-name&gt;\n</code></pre>"},{"location":"learn/dataconverter-and-readers.html#merge-partial-nexus-files-into-one","title":"Merge partial NeXus files into one","text":"<pre><code>user@box:~$ dataconverter --nxdl nxdl partial1.nxs partial2.nxs\n</code></pre>"},{"location":"learn/dataconverter-and-readers.html#map-an-hdf5-filejson-file","title":"Map an HDF5 file/JSON file","text":"<pre><code>user@box:~$ dataconverter --nxdl nxdl any_data.hdf5 --mapping my_custom_map.mapping.json\n</code></pre> <p>You can find actual examples with data files at <code>examples/json_map</code>.</p>"},{"location":"learn/dataconverter-and-readers.html#example-data-for-testing-and-development-purposes","title":"Example data for testing and development purposes","text":"<p>Before using your own data we strongly encourage you to download a set of open-source test data for testing the pynxtools readers andreader  plugins. For this purpose, pynxtools and its plugins come  with <code>examples</code> and <code>test</code> directories including reader-specific examples. These examples can be used for downloading test data and use specific readers as a standalone converter to translate given data into a NeXus/HDF5 file.</p> <p>Once you have practized with these tools how to convert these examples, feel free to use the tools for converting your own data. You should feel invited to contact the respective corresponding author(s) of each reader if you run into issues with the reader or feel there is a necessity to include additional data into the NeXus file for your respective application.</p> <p>We are looking forward to learning from your experience and learn from your use cases. You can find the contact persons in the respective README.md of each reader (plugin).</p>"},{"location":"learn/multi-format-reader.html","title":"The MultiFormatReader as a reader superclass","text":"<p>There are three options for building a new <code>pynxtools</code> reader:</p> <ol> <li>build the reader from scratch</li> <li>inherit and extend the <code>BaseReader</code></li> <li>inherit and extend the <code>MultiFormatReader</code></li> </ol> <p>While option 1 is generally not recommended, inheriting and extending the <code>BaseReader</code> has traditionally been the default solution for all existing pynxtools readers and reader plugins. The <code>BaseReader</code>, which is an abstract base class, has an essentially empty <code>read</code> function and is  thus only helpful for implementing the correct input/output design of the <code>read</code> function of any reader which is implemented off of it.</p> <p>While building on the <code>BaseReader</code> allows for the most flexibility, in most cases it is desirable to implement a reader that can read in multiple file formats and then populate the NeXus file based on the read data, in compliance with a NeXus application definition. For this purpose, <code>pynxtools</code> has the <code>MultiFormatReader</code>, which can be readily extended for any new data.</p> <p>Here, we will explain the inner workings of the <code>MultiFormatReader</code>. Note that there is also a how-to guide on how to implement a new reader off of the <code>MultiFormatReader</code> using a concrete example. In case you simply want to use the <code>MultiFormatReader</code> without understanding its inner logic, we recommend you start there.</p>"},{"location":"learn/multi-format-reader.html#the-basic-structure","title":"The basic structure","text":"<p>For extending the <code>MultiFormatReader</code>, the following basic structure must be implemented: multi/reader.py<pre><code>\"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\nfrom typing import Tuple, Any\n\nfrom pynxtools.dataconverter.readers.base.reader import MultiFormatReader\n\nclass MyDataReader(MultiFormatReader):\n    \"\"\"MyDataReader implementation for the DataConverter to convert mydata to NeXus.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.extensions = {\n            \".yml\": self.handle_eln_file,\n            \".yaml\": self.handle_eln_file,\n            \".json\": self.set_config_file,\n            # Here, one must add functions for handling any other file extension(s)\n        }\n# This has to be set to allow the convert script to use this reader. Set it to \"MyDataReader\".\nREADER = MyDataReader\n</code></pre></p> <p>In order to understand the capabilities of the <code>MultiFormatReader</code> and which methods need to be implemented when extending it, we will have a look at its <code>read</code> method: multi/reader.py<pre><code>def read(\n    self,\n    template: dict = None,\n    file_paths: Tuple[str] = None,\n    objects: Optional[Tuple[Any]] = None,\n    **kwargs,\n) -&gt; dict:\n    self.kwargs = kwargs\n    self.config_file = self.kwargs.get(\"config_file\", self.config_file)\n    self.overwrite_keys = self.kwargs.get(\"overwrite_keys\", self.overwrite_keys)   \n</code></pre></p>"},{"location":"learn/multi-format-reader.html#template-initialization-and-processing-order","title":"Template initialization and processing order","text":"<p>An empty <code>Template</code> object is initialized that later gets filled from the data files later. multi/reader.py<pre><code>    template = Template(overwrite_keys=self.overwrite_keys)\n\n    def get_processing_order(path: str) -&gt; Tuple[int, Union[str, int]]:\n        \"\"\"\n        Returns the processing order of the file.\n        \"\"\"\n        ext = os.path.splitext(path)[1]\n        if self.processing_order is None or ext not in self.processing_order:\n            return (1, ext)\n        return (0, self.processing_order.index(ext))\n\n    sorted_paths = sorted(file_paths, key=get_processing_order)\n</code></pre> If the reader has a <code>self.processing_order</code>, the input files get sorted in this order. If <code>self.overwrite_keys</code> is True, later files get precedent. For example, if <code>self.processing_order = [\".yaml\", \".hdf5\"]</code>, any values coming from HDF5 files would overwrite values from the YAML files.</p>"},{"location":"learn/multi-format-reader.html#reading-of-input-files","title":"Reading of input files","text":"<p>multi/reader.py<pre><code>    for file_path in sorted_paths:\n        extension = os.path.splitext(file_path)[1].lower()\n        if extension not in self.extensions:\n            logger.warning(\n                f\"File {file_path} has an unsupported extension, ignoring file.\"\n            )\n            continue\n        if not os.path.exists(file_path):\n            logger.warning(f\"File {file_path} does not exist, ignoring entry.\")\n            continue\n\n        template.update(self.extensions.get(extension, lambda _: {})(file_path))\n</code></pre> This parts reads in the data from all data files. The <code>MultiFormatReader</code> has an <code>extensions</code> property, which is a dictionary that for each file extension calls a function that reads in data from files with that extension. If the reader shall handle e.g. an HDF5 file, a method for handling this type of file should be added, i.e., <code>self.extensions[\".hdf5\"] = self.handle_hdf5</code>. Note that these methods should also implement any logic depending on the provided data, i.e., it may not be sufficient to rely on the filename suffix, but the reader may also need to check for different file versions, binary signature, mimetype, etc.</p> <p>Any of these methods should take as input only the file path, e.g. multi/reader.py<pre><code>def handle_eln_file(self, file_path: str) -&gt; Dict[str, Any]\n</code></pre> These methods must return a dictionary. One possibility is to return a dictionary that directly fills the template (see the <code>template.update</code> call above) with the data from the file. Another option is to return an empty dictionary (i.e., not fill the template at this stage) and only later fill the template from a config file (see below).</p> <p>Note that for several input formats, standardized parser functions already exist within the <code>MultiFormatReader</code>. For example, YAML files can be parsed using the <code>pynxtools.dataconverter.readers.utils.parse_yml</code> function.</p>"},{"location":"learn/multi-format-reader.html#setting-default-values-in-the-template","title":"Setting default values in the template","text":"<p>multi/reader.py<pre><code>    template.update(self.setup_template())\n</code></pre> Next, the <code>setup_template</code> method can be implemented, which is used to populate the template with initial data that does not come from the files themselves. This may be used to set fixed information, e.g., about the reader. As an example, <code>NXentry/program_name</code> (which is defined as the name of program used to generate the NeXus file) scan be set to <code>pynxtools-plugin</code> by making <code>setup_template</code> return a dictionary of the form <pre><code>{\n  \"/ENTRY[my_entry]/program_name\": \"pynxtools-plugin\",\n  \"/ENTRY[my_entry]/program_name/@version\": \"v0.1.0\"\n}\n</code></pre></p>"},{"location":"learn/multi-format-reader.html#handling-objects","title":"Handling objects","text":"<p>multi/reader.py<pre><code>    if objects is not None:\n        template.update(self.handle_objects(objects))\n</code></pre> Aside from data files, it is also possible to directly pass any Python objects to the <code>read</code> function (e.g., a numpy array with measurement data). In order to exploit this, the <code>handle_objects</code> method must implemented, which should return a dictionary that populates the template.</p>"},{"location":"learn/multi-format-reader.html#parsing-the-config-file","title":"Parsing the config file","text":"<p>multi/reader.py<pre><code>    if self.config_file is not None:\n        self.config_dict = parse_flatten_json(\n            self.config_file, create_link_dict=False\n        )\n</code></pre> Next up, we can make use of the config file, which is a JSON file that tells the reader which input data to use to populate the template. In other words, the config.json is used for ontology mapping between the input file paths and the NeXus application definition. Essentially, the config file should contain all keys that are present in the NXDL. A subset of a typical config file may look like this: <pre><code>{\n  \"/ENTRY/title\": \"@attrs:metadata/title\", \n  \"/ENTRY/USER[user]\": {\n    \"name\": \"my_name\",\n  }, \n  \"/ENTRY/INSTRUMENT[instrument]\": {\n    \"name\":\"@eln\",\n    \"temperature_sensor\": {\n      \"value\": \"@attrs:metadata/temp\",\n      \"value/@units\": \"K\"\n    }\n  },\n  \"/ENTRY/SAMPLE[sample]\": {\n    \"temperature_env\": {\n      \"temperature_sensor\": \"@link:/entry/instrument/temperature_sensor\"\n    }\n  },  \n  \"/ENTRY/data\": {\n    \"@axes\": \"@data:dims\",\n    \"AXISNAME_indices[@*_indices]\": \"@data:*.index\",\n    \"@signal\": \"data\",\n    \"data\": \"@data:mydata\",\n  }\n}\n</code></pre> Here, the <code>parse_flatten_json</code> method is used that allows us to write the config dict in the structured manner above and internally flattens it (so that it has a similar structure as the Template).</p> <p>In the config file, one can</p> <ol> <li>hard-code values (like the unit <code>\"K\"</code> in <code>\"/ENTRY/INSTRUMENT[instrument]/temperature_sensor/value/@units\"</code>) or</li> <li>tell the reader where to search for data using the <code>@</code>-prefixes. For more on these prefixes, see below.</li> </ol> <p>Note that in order to use a <code>link_callback</code> (see below), <code>create_link_dict</code> must be set to <code>False</code>, which means that at this stage, config values of the form <code>\"@link:\"/path/to/source/data\"</code> get NOT yet converted to <code>{\"link\": \"/path/to/source/data\"}</code>.</p>"},{"location":"learn/multi-format-reader.html#data-post-processing","title":"Data post processing","text":"<p>multi/reader.py<pre><code>   self.post_process()\n</code></pre> In case there is the need for any post-processing on the data and/or config dictionary after they have been read, the <code>post_process</code> method can be implemented. For example, this can be helpful if there are multiple entities of a given NX_CLASS (for example, multiple detectors) on the same level and the config dict shall be set up to fill the template with all of these entities.</p>"},{"location":"learn/multi-format-reader.html#filling-the-template-from-the-read-in-data","title":"Filling the template from the read-in data","text":"<p>multi/reader.py<pre><code>    if self.config_dict:\n        suppress_warning = kwargs.pop(\"suppress_warning\", False)\n        template.update(\n            fill_from_config(\n                self.config_dict,\n                self.get_entry_names(),\n                self.callbacks,\n                suppress_warning=suppress_warning,\n            )\n        )\n\n    return template\n</code></pre> As a last step, the template is being filled from the config dict using the data. If there is more than one entry, the <code>get_entry_names</code> method must be implemented, which shall return a list of all entry names. The <code>fill_from_config</code> method iterates through all of the them and replaces the generic <code>/ENTRY/</code> in the config file by keys of the form <code>/ENTRY[my-entry]/</code> to fill the template.</p> <p>Here, we are using callbacks, which are used to bring in data based on <code>@</code>-prefixes in the config file. These are defined in the reader's <code>__init__</code> call using the <code>pynxtools.dataconverter.readers.multi.ParseJsonCallbacks</code> class: multi/reader.py<pre><code>self.callbacks = ParseJsonCallbacks(\n    attrs_callback=self.get_attr,\n    data_callback=self.get_data,\n    eln_callback=self.get_eln_data,\n    dims=self.get_data_dims,\n)\n</code></pre> The <code>ParseJsonCallbacks</code> class has an attribute called <code>special_key_map</code> that makes use of these callbacks to populate the template based on the starting prefix of the config dict value: multi/reader.py<pre><code>self.special_key_map = {\n    \"@attrs\": attrs_callback if attrs_callback is not None else self.identity,\n    \"@link\": link_callback if link_callback is not None else self.link_callback,\n    \"@data\": data_callback if data_callback is not None else self.identity,\n    \"@eln\": eln_callback if eln_callback is not None else self.identity,\n}\n</code></pre> That means, if the config file has an entry <code>{\"/ENTRY/title\": \"@attrs:metadata/title\"}</code>, the <code>get_attr</code> method of the reader gets called and should return an attribute from the given path, i.e., in this case from <code>metadata/title</code>.</p> <p>By default, the MultiFormatReader supports the following special prefixes:</p> <ul> <li><code>@attrs</code>: To get metadata from the read-in experiment file(s). You need to implement the <code>get_attr</code> method in the reader.</li> <li><code>@data</code>: To get measurement data from the read-in experiment file(s). You need to implement the <code>get_data</code> method in the reader.</li> <li><code>@eln</code>: To get metadata from addtional ELN files. You need to implement the <code>get_eln_data</code> method in the reader.</li> <li><code>@link</code>: To implement a link between two entities in the NeXus file. By default, the link callback returns a dict of the form {\"link\": value.replace(\"/entry/\", f\"/{self.entry_name}/\")}, i.e., a generic <code>/entry/</code> get replaced by the actual <code>entry_name</code>.</li> </ul> <p>The destinction between data and metadata is somewhat arbitrary here. The reason to have both of these prefixes is to have different methods to access different parts of the read-in data. For example, <code>@attrs</code> may just access key-value pairs of a read-in dictionary, whereas <code>@data</code> can handle different object types, e.g. xarrays. The implementation in the reader decides how to distinguish data and metadata and what each of the callbacks shall do.</p> <p>In addition, the reader can also implement the <code>get_data_dims</code> method, which is used to return a list of the data dimensions (see below for more details).</p> <p>All of <code>get_attr</code>, <code>get_data</code>, and <code>get_eln_data</code>  (as well as any similar method that might be implemented) should have the same call signature: <pre><code>def get_data(self, key: str, path: str) -&gt; Any:\n</code></pre> Here, <code>key</code> is the config dict key (e.g., <code>\"/ENTRY[my-entry]/data/data\"</code>) and path is the path that comes after the prefix in the config file. In the example config file above, <code>path</code> would be <code>mydata</code>. With these two inputs, the reader should be able to return the correct data for this template key.</p>"},{"location":"learn/multi-format-reader.html#special-rules","title":"Special rules","text":"<ul> <li> <p>Lists as config value: It is possible to write a list of possible configurations of the sort   <pre><code>\"/ENTRY/title\":\"['@attrs:my_title', '@eln', 'no title']\"\n</code></pre>   The value must be a string which can be parsed as a list, with each item being a string itself. This allows to provide different options depending if the data exists for a given callback. For each list item , it is checked if a value can be returned and if so, the value is written. In this example, the converter would check (in order) the <code>@attrs</code> (with path <code>\"my_title\"</code>) and <code>@eln</code> (with path <code>\"\"</code>) tokens and write the respective value if it exists. If not, it defaults to \"no title\".   This concept can be particularly useful if the same config file is used for multiple measurement configurations, where for some setup, the same metadata may or may not be available.</p> <p>Note that if this notation is used, it may be helpful to pass the <code>suppress_warning</code> keyword as <code>True</code> to the read function. Otherwise, there will be a warning for every non-existent value.</p> </li> <li> <p>Wildcard notation: There exists a wildcard notation (using <code>*</code>)   <pre><code>\"/ENTRY/data/AXISNAME[*]\": \"@data:*.data\",\n</code></pre>   that allows filling multiple fields of the same type from a list of dimensions. This can be particularly helpful for writing <code>DATA</code> and <code>AXISNAME</code> fields that are all stored under similar paths in the read-in data.   For this, the <code>get_data_dims</code> method needs to be implemented. For a given path, it should return a list of all data axes available to replace the wildcard.</p> <p>The same wildcard notation can also be used within a name to repeat entries with different names (e.g., field_*{my, name, etc} is converted into three keys with * replaced by my, name, etc, respectively). As an example, for multiple lenses and their voltage readouts, one could write:   <pre><code>\"LENS_EM[lens_*{A,B,Foc}]\": {\n  \"name\": \"*\",\n  \"voltage\": \"@attrs:metadata/file/Lens:*:V\",\n  \"voltage/@units\": \"V\"\n},\n</code></pre>   which would write <code>NXlens_em</code> instances named <code>lens_A</code>, <code>lens_B</code>, and <code>lens_Foc</code>.</p> </li> <li> <p>Required fields in optional groups: There will sometimes be the situation that there is an optional NeXus group in an application definition, that (if implemented) requires some sub-element. As an example, for the instrument's energy resolution, the only value expected to come from a data source is the <code>resolution</code>, whereas other fields are hardcoded.   <pre><code>\"ENTRY/INSTRUMENT[instrument]/energy_resolution\": {\n  \"resolution\": \"@attrs:metadata/instrument/electronanalyser/energy_resolution\",\n  \"resolution/@units\": \"meV\",\n  \"physical_quantity\": \"energy\"\n}\n</code></pre>   Now, if there is no data for <code>@attrs:metadata/instrument/electronanalyser/energy_resolution</code> available in a dataset, this will be skipped by the reader, and not available, yet the other entries are present. During validation, this means that the required field <code>resolution</code> of the optional group <code>energy_resolution</code> is not present, and thus a warning or error would be raised:   <pre><code>LookupError: The data entry, /ENTRY[entry]/INSTRUMENT[instrument]/ELECTRONANALYSER[electronanalyser]/energy_resolution/physical_quantity, has an optional parent, /ENTRY[entry]/INSTRUMENT[instrument]/ELECTRONANALYSER[electronanalyser]/energy_resolution, with required children set. Either provide no children for /ENTRY[entry]/INSTRUMENT[instrument]/ELECTRONANALYSER[electronanalyser]/energy_resolution or provide all required ones.\n</code></pre></p> <p>To circumvent this problem, there exists a notation using the <code>\"!\"</code> prefix. If you write <pre><code>\"ENTRY/INSTRUMENT[instrument]/energy_resolution/resolution\": \"!@attrs:metadata/instrument/electronanalyser/energy_resolution\"\n</code></pre> the whole parent group <code>/ENTRY/INSTRUMENT[instrument]/energy_resolution</code> will not be written in case that there is no value for <code>@attrs:metadata/instrument/electronanalyser/energy_resolution\"</code>, thus preventing the aforementioned error.</p> </li> </ul>"},{"location":"learn/multiple-appdefs.html","title":"Multiple Application Definitions in NeXus","text":"<p>Work in progress</p> <p>This part of the documentation is still being written and it might be confusing or incomplete.</p> <p>This tutorial showcases how to employ multiple application definitions in NeXus for creating a file that conforms to various definitions simultaneously.</p> <p>Prerequisites</p> <p>Familiarity with the basics of NeXus and its application definitions is required. For an introduction to NeXus, please refer to the basic documentation.</p> <p>In a laboratory setting, the data we collect can vary significantly depending on the experiment's specific setup. Consider, for instance, an experiment characterized using the <code>NXexperiment</code> application definition. Suppose we want to enhance this experiment by incorporating energy resolution details. A straightforward approach might involve creating a specialized sub-application definition, like <code>NXexperiment_energy_resolved</code>, to include metadata about the experiment's energy resolution.</p> <p>While this method is effective for initial expansions of the metadata, it becomes cumbersome when trying to merge multiple enhancements into a single application definition. Imagine we wish to integrate additional elements that provide time resolution data for our experiment.  We need to create three sub application definitions to reflect all combinations: <code>NXexperiment_time_resolved</code>, <code>NXexperiment_energy_resolved</code> and <code>NXexperiment_energy_time_resolved</code>. For three experimental facets we already need 7 sub application definitions. An additional problem is that we have to repeat the whole procedure for all experiments we like to add the specific traits to. So if we have three different parent application definitions we already need to create 9 sub application definitions just to add energy and time resolution.</p> <p>The solution for this problem is to specify multiple application definitions while writing a NeXus file to follow different traits of the experiment. This allows us to simply create <code>NXtime_resolved</code> and <code>NXenergy_resolved</code> and combine it with any experiment we want to use it with. This comes, however, with a few drawbacks. One of them is that it is currently not possible to write a file which wants to use two different application definitions which have conflicting fields. While this is generally possible in the framework of NeXus, since every application definition creates their own namespace, this is not supported when the paths are reduced to entry path notation.</p> <p>ToDo: - Make an example of NXexperiment, NXtime_resolved and NXenergy_resolved and show how it is combined into the instance path. - Also show this for a conflict. Compare concept path (no problem) to instance path (colliding). - Write a part how it is described in the file that it follows two appdefs <code>/entry/definitions</code> as array containing both appdefs. - Explain that this is no problem with the expanded concept path notation but fails when we only use the instance path. - Explain the reader concept: One reader for one appdef, then pynxtools will figure out how to combine them (this is domain knowledge for the FAIRmat reader -&gt; will be different when a read/write tool is written somewhere else).</p>"},{"location":"learn/nexus-primer.html","title":"A primer on NeXus","text":"<p>Work in progress</p> <p>This part of the documentation is still being written and it might be confusing or incomplete.</p> <p>NeXus is is a description of a common data exchange format initially developed for neutron, X-ray, and muon experiments. Within FAIRmat we extensively extended the format to cover a range of experiments with major support for APM, ARPES, XPS, and optical spectroscpy but we also give advice and guidance for developing standards for other formats as well.</p> <p>NeXus as a tool for FAIR data</p> <p>NeXus is supported be the research data management platform NOMAD. Experimental data following an NeXus application definition can easily be uploaded and is recognized by NOMAD's search system. If you want to learn more about uploading NeXus data to NOMAD, please refer to the NeXus to nomad tutorial of this documentation. Accordingly, if you want to build data according to the FAIR principles, you can think of NeXus fulfilling the interoperability and reproducibility part and a research data management platform like NOMAD the findable and accessible part.</p>"},{"location":"learn/nexus-primer.html#what-is-nexus","title":"What is NeXus?","text":"<p>Sometimes, NeXus is seen as writing data to some form of file in hdf5 format. While this is partly true, NeXus is independent of the actual storage format but is typically written into an hdf5 file.</p> <p>But what is NeXus then? It is the conceptual layer above the file structure. It is a contract on which data has to be present and how to name them in a given dataset. Hence, using NeXus participates in making data FAIR. It especially covers the interoperability and reproducibility part of research data.</p> <p>NeXus path notations</p> <p>There are several methods for referencing concepts or data paths within NeXus:</p> <ul> <li> <p>Concept Path Notation: This notation describes the hierarchical structure of NeXus concepts using class names. For example, <code>NXexperiment:/NXentry/NXinstrument/NXdetector</code> indicates the creation of a new NXdetector class within the NXexperiment concept. This path typically forms automatically when an application definition extends a base class's fields.</p> </li> <li> <p>Instance Path Notation: It represents the actual location of a field or group in a NeXus data instance (e.g., an HDF5 file). An example is <code>my_file.nxs:/entry/instrument/detector</code>.</p> </li> <li> <p>Combined Notation: This combines concept and instance paths. For example, <code>NXexperiment:/NXentry[my_file.nxs:entry]/NXinstrument[instrument]/NXdetector[detector]</code>. Here, concept paths are outside and instance paths within square brackets. The leftmost entries may include the NeXus class or file reference.</p> </li> <li> <p>Appdef Notation: This format is used in application definitions, where uppercase indicates a selectable name and lowercase a fixed name. Examples include <code>NXexperiment:ENTRY[my_experiment.nxs:entry]/INSTRUMENT[instrument]/DETECTOR[detector]</code> and <code>NXexperiment:ENTRY[my_experiment.nxs:entry]/my_INSTRUMENT[my_instrument]/DETECTOR[detector]</code>.</p> </li> </ul>"},{"location":"learn/nexus-rules.html","title":"Rules for storing data in NeXus","text":"<p>There are several rules which apply for storing single data items in NeXus. There exists a summary in the NeXus documentation outlining most of these rules. However, to guide data providers even further, we have compiled here additional information and explanations.</p>"},{"location":"learn/nexus-rules.html#namefitting","title":"Namefitting","text":"<p>In general, the names of NeXus group and field items are validated according to the boundaries outlined in the Rules for Storing Data Items in NeXus Files, section \"NXDL group and field names\": - Recommended names   - lower case words separated by underscores and, if needed, with a trailing number</p> <ul> <li>Allowed names</li> <li>any combination of upper and lower case letter, numbers, underscores and periods, except that periods cannot be at the start or end of the string</li> <li>This statement is equivalent to matching  this regular expression (named <code>validItemName</code> in the nxdl.xsd) XML Schema file:   <pre><code>^[a-zA-Z0-9_]([a-zA-Z0-9_.]*[a-zA-Z0-9_])?$\n</code></pre></li> <li>Invalid names:</li> <li>any name not matching this <code>validItemName</code> regular expression</li> </ul> <p>Note that this explicitly also means that it is not allowed to have whitespace (including \" \") in NeXus names.</p> <p>In NeXus base classes and application definitions, there are two options for defining a concept name. If the group or field in the definition is lowercase, that means that any instance must have the exact same (fixed) name. As an example, if there is a field called <code>my_field</code> in an application definition, the only allowed name in a file would be <code>my_field</code>.</p> <p>Aside from this lower case notation, there is also the option to allow for selectable names. This is achieved by uppercase notation. As an example, if a field in an application definition is called <code>FIELD</code>, the name can be any name as long as it maches the regular expression above. For example, <code>field</code>, <code>field0</code>, <code>any_other_name</code> would be allowed names, while <code>any other name</code> would not be allowed.</p> <p>There is also the possibility of mixed lowercase and uppercase notation in base classes and application definitions. For example, there might be a <code>userID(NXuser)</code> group. In this case, allowed names include any name that start with <code>user</code>, e.g., <code>user0</code>, <code>user_abcde</code>, as long as the part that replaces the docstring is still valid according to the regex above. Note that here it is also not allowed to write <code>user</code> without replacing the uppercase part of the name.</p> <p>The validation of names is performed by namefitting, i.e., fitting the name that is used by the data provider to the name given in the base class / application definitions.</p> <p>A python implementation of this process can be found in this function. This function returns twice the length for an exact match, otherwise the number of matching characters (case insensitive) or zero, if <code>name_any</code> is set to True, is returned. This is also the function that is used in the validation implemented in pynxtools.</p>"},{"location":"learn/nexus-rules.html#special-rules-for-names-and-namefitting","title":"Special rules for names and namefitting","text":"<p>Aside from these general rules, there are a number of special rules for NeXus names that need to be considered:</p> <ul> <li> <p>There is a set of UPPERCASE reserved words (like <code>BLUESKY_</code>, <code>DECTRIS_</code>, <code>IDF_</code>, etc.) that are reserved for certain projects and communities. These are prefixes (typically written as uppercase + undersorce) that cannot be overwritten by namefitting. For the full list, see Rules for Storing Data Items in NeXus Files, section \"Reserved prefixes\".</p> </li> <li> <p>There is also a set of reserved suffixes that are used to give additional information for a group or field. For the full list, see Rules for Storing Data Items in NeXus Files, section \"Reserved suffixes\".</p> </li> <li> <p>Additionally to namefitting, data annotation can use further information. For example, in case of NXdata, the axes listed among the <code>@axes</code> shall fit to any instances of <code>AXISNAME</code> and data objects listed in <code>@signal</code> or <code>@auxiliary_signals</code> shall fit to instances of <code>DATA</code>. Such rules are typically given in the base classes (e.g., see here for NXdata). Any tool that makes use of the base classes should implement these special rules in its validation procedure. As an example, pynxtools has a special function for handling NXdata.</p> </li> </ul>"},{"location":"learn/nexus-validation.html","title":"NeXus validation","text":"<p>Work in progress</p> <p>One of the main advantages of using pynxtools is that it comes with its own validation tools. That is, it can be used to validate that a given NeXus/HDF5 file is compliant with a NeXus application definition.</p>"},{"location":"learn/nexus-validation.html#as-part-of-the-dataconverter","title":"As part of the dataconverter","text":"<p>During data conversion, before writing the HDF5 file, the data is first checked against the provided application definition.</p>"},{"location":"learn/nexus-validation.html#read_nexus-nexus-file-reader-and-debugger","title":"read_nexus: NeXus file reader and debugger","text":"<p>This utility outputs a debug log for a given NeXus file by annotating the data and metadata entries with the schema definitions from the respective NeXus base classes and application definitions to which the file refers to. See here for the API documentation.</p> <p>The following example dataset can be used to test the <code>read_nexus</code> module: src/pynxtools/data/201805_WSe2_arpes.nxs.</p> <p>This is an angular-resolved photoelectron spectroscopy (ARPES) dataset that is formatted according to the NXarpes application definition of NeXus.</p>"},{"location":"learn/nexus-validation.html#using-a-different-set-of-nexus-definitions","title":"Using a different set of NeXus definitions","text":"<p>The environment variable \"NEXUS_DEF_PATH\" can be set to a directory which contains the NeXus definitions as NXDL XML files. If this environment variable is not defined, the module will use the definitions in its bundle (see <code>src/pynxtools/definitions</code>)._</p> <p>The environment variable can be set as follows: <pre><code>export 'NEXUS_DEF_PATH'=&lt;folder_path_that_contains_nexus_defs&gt;\n</code></pre></p>"},{"location":"learn/nexus-validation.html#a-note-to-windows-users","title":"A note to Windows users","text":"<p>If you run <code>read_nexus</code> from <code>git bash</code>, you need to set the environmental variable <code>MSYS_NO_PATHCONV</code> to avoid the path translation in Windows Git MSys. The easiest way is to prefix the <code>read_nexus</code> call with <code>MSYS_NO_PATHCONV=1</code>:</p> <pre><code>MSYS_NO_PATHCONV=1 read_nexus -c /NXarpes/ENTRY/INSTRUMENT/analyser\n</code></pre> <p>This workaround was tested with Windows 11, but should very likely also work with Windows 10 and lower.</p>"},{"location":"learn/nexus-validation.html#other-approaches-not-part-of-pynxtools","title":"Other approaches (not part of pynxtools)","text":"<p>Aside from the tools we developed within FAIRmat, the official NeXus website listed two more programs for the validation of NeXus files:</p> <ol> <li>nxvalidate</li> <li>punx</li> </ol>"},{"location":"reference/built-in-readers.html","title":"Built-in readers","text":"<p>There exists a number of readers directly in pynxtools. These are typically used either as superclasses for new reader implementations or for generic reading purposes not directly related to any specific technique.</p>"},{"location":"reference/built-in-readers.html#the-basereader","title":"The BaseReader","text":"<p>This is the most simple reader, which is an abstract base class, on top of which a new reader implementation can build. It has an essentially empty read function and is thus only helpful for implementing the correct input/ouput design of the <code>read</code> function of any reader that is inheriting from this base reader.</p>"},{"location":"reference/built-in-readers.html#the-multiformatreader","title":"The MultiFormatReader","text":"<p>Another reader that can act as the basis for any reader implementation is the <code>MultiFormatReader</code>, which can be used to implement a reader that can read in multiple file formats and then populate the NeXus file using the read data. Note that this reader has a lot of already built-in functionality, which is extensively described here. There is also a how-to guide on how to implement a new reader off of the <code>MultiFormatReader</code> using a concrete example.</p>"},{"location":"reference/built-in-readers.html#the-jsonmapreader","title":"The JsonMapReader","text":"<p>This reader is designed to allow users of <code>pynxtools</code> to convert their existing data with the help of a map file. The map file tells the reader which concept and instance data to pick from the data files and how to convert these to NeXus files. The following formats are supported as input files:</p> <ul> <li>HDF5</li> <li>JSON</li> <li>Python Dict Objects pickled with pickle. These can contain xarray.DataArray objects as well as regular Python types and Numpy types. Note that while it is supported, we strongly recommend note to use pickle due to its known security concerns.</li> </ul> <p>It accepts any XML file that follows the NXDL schema definition language file as long as your mapping file contains all the required fields. Please use the <code>--generate-template</code> function of the <code>dataconverter</code> to create a <code>.mapping.json</code> file:</p> <pre><code>user@box:~$ dataconverter --nxdl NXmynxdl --generate-template &gt; mynxdl.mapping.json\n</code></pre>"},{"location":"reference/built-in-readers.html#the-mappingjson-file","title":"The mapping.json file","text":"<p>This file is designed to let you fill in the requirements of a NeXus Application Definition without writing any code. If you already have data in the formats listed above, you just need to use this mapping file to help the dataconverter pick your data correctly.</p> <p>The mapping files will always be based on the template the dataconverter generates. See above on how to generate a mapping file. The right hand side values of the template keys are what you can modify. These keys are called NeXus template paths, because they combine the actual path that will be used in the HDF5 hierarchy with additional NeXus datatype hints to guide the dataconverter to add NX_class annotations.</p> <p>Here are the three different ways you can fill the right hand side of the template keys: * Write the nested path in your datafile. This is indicated by a leading <code>/</code> before the word <code>entry</code> to make <code>/entry/data/current_295C</code> below. Example:</p> <p><pre><code>  \"/ENTRY[entry]/DATA[data]/current_295C\": \"/entry/data/current_295C\",\n  \"/ENTRY[entry]/NXODD_name/posint_value\": \"/a_level_down/another_level_down/posint_value\",\n</code></pre> Here, <code>\"/entry/data/current_295C\"</code> is the path in the original HDF5 file, while the key shown here is the template path (see above).</p> <ul> <li>Write the values directly in the mapping file for missing data from your data file.</li> </ul> <pre><code>  \"/ENTRY[entry]/PROCESS[process]/program\": \"Bluesky\",\n  \"/ENTRY[entry]/PROCESS[process]/program/@version\": \"1.6.7\"\n</code></pre> <ul> <li>Write JSON objects with a link key. This follows the same link mechanism that the dataconverter implements. In the context of this reader, you can only use external links to your data files. In the example below, <code>current.nxs</code> is an already existing HDF5 file that we link to in our new NeXus file without copying over the data. The format is as follows: <code>\"link\": \"&lt;filename&gt;:&lt;path_in_file&gt;\"</code> Note: This only works for HDF5 files currently.</li> </ul> <pre><code>  \"/ENTRY[entry]/DATA[data]/current_295C\": {\"link\": \"current.nxs:/entry/data/current_295C\"},\n  \"/ENTRY[entry]/DATA[data]/current_300C\": {\"link\": \"current.nxs:/entry/data/current_300C\"},\n</code></pre>"},{"location":"reference/built-in-readers.html#examples","title":"Examples","text":""},{"location":"reference/built-in-readers.html#basic-mappings","title":"Basic mappings","text":"<p>There are some example files you can use:</p> <p>data.mapping.json</p> <p>data.json</p> <pre><code>user@box:~$ dataconverter --nxdl NXtest data.json --mapping data.mapping.json\n</code></pre>"},{"location":"reference/built-in-readers.html#example-with-hdf5-files","title":"Example with HDF5 files","text":"<p>You can find example data files for using the mapping with HDF5 files at <code>examples/json_map</code>.</p> <p>The example can be run by calling</p> <pre><code>user@box:~$ dataconverter --nxdl nxdl any_data.hdf5 --mapping my_custom_map.mapping.json\n</code></pre>"},{"location":"reference/built-in-readers.html#the-yamljsonreader","title":"The YamlJsonReader","text":"<p>Work in progress</p>"},{"location":"reference/built-in-readers.html#installation","title":"Installation","text":"<p>Each of the built-in readers are shipped/installed with the main <code>pynxtools</code> package. Hence, these readers are available after pip installation: <pre><code>user@box:~$ pip install pynxtools\n</code></pre></p>"},{"location":"reference/cli-api.html","title":"API for command line tools","text":"<p><code>pynxtools</code> supports a number of command line applications. This page provides documentation for their current API.</p>"},{"location":"reference/cli-api.html#data-conversion","title":"Data conversion","text":"<p>Note that simply calling <code>dataconverter</code> defaults to <code>dataconverter convert</code>.</p>"},{"location":"reference/cli-api.html#dataconverter","title":"dataconverter","text":"<p>Usage:</p> <pre><code>dataconverter [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code> <p>Subcommands</p> <ul> <li>convert: This command allows you to use the converter functionality of the dataconverter.</li> <li>generate-template: Generates and prints a template to use for your nxdl.</li> <li>get-readers: Prints a list of all installed readers.</li> </ul>"},{"location":"reference/cli-api.html#dataconverter-convert","title":"dataconverter convert","text":"<p>This command allows you to use the converter functionality of the dataconverter.</p> <p>Usage:</p> <pre><code>dataconverter convert [OPTIONS] [FILES]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--input-file</code> text Deprecated: Please use the positional file arguments instead. The path to the input data file to read. Repeat for more than one file. default=[] This option is required if no '--params-file' is supplied. <code>[]</code> <code>--reader</code> choice (<code>example</code> | <code>json_map</code> | <code>json_yml</code> | <code>multi</code>) The reader to use. Examples are json_map or readers from a pynxtools plugin. default='json_map' This option is required if no '--params-file' is supplied. <code>json_map</code> <code>--nxdl</code> text The name of the NeXus application definition file to use without the extension nxdl.xml. This option is required if no '--params-file' is supplied. None <code>--output</code> text The path to the output NeXus file to be generated. default='output.nxs' <code>output.nxs</code> <code>--params-file</code> filename Allows to pass a .yaml file with all the parameters the converter supports. None <code>--ignore-undocumented</code> boolean Ignore all undocumented fields during validation. <code>False</code> <code>--fail</code> boolean Fail conversion and don't create an output file if the validation fails. <code>False</code> <code>--skip-verify</code> boolean Skips the verification routine during conversion. <code>False</code> <code>--mapping</code> text Takes a .mapping.json file and converts data from given input files. None <code>-c</code>, <code>--config</code> file A json config file for the reader None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli-api.html#dataconverter-generate-template","title":"dataconverter generate-template","text":"<p>Generates and prints a template to use for your nxdl.</p> <p>Usage:</p> <pre><code>dataconverter generate-template [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--nxdl</code> text The name of the NXDL file to use without extension. For example: NXmpes _required <code>--required</code> boolean Use this flag to only get the required template. <code>False</code> <code>--pythonic</code> boolean Prints a valid Python dictionary instead of JSON <code>False</code> <code>--output</code> path Writes the output into the filepath provided. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli-api.html#dataconverter-get-readers","title":"dataconverter get-readers","text":"<p>Prints a list of all installed readers.</p> <p>Usage:</p> <pre><code>dataconverter get-readers [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli-api.html#nexus-file-validation","title":"NeXus file validation","text":""},{"location":"reference/cli-api.html#read_nexus","title":"read_nexus","text":"<p>Functionality to extract documentation and concept definition information about the individual parts of a NeXus/HDF5 file.</p> <p>Usage:</p> <pre><code>read_nexus [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>-f</code>, <code>--nexus-file</code> text NeXus file with extension .nxs. None <code>-d</code>, <code>--documentation</code> text Definition path in nexus output (.nxs) file. Returns debug log relevant with that definition path. Example: /entry/data/delays None <code>-c</code>, <code>--concept</code> text Concept path from application definition file (.nxdl,xml). Finds out all the available concept definition (IS-A realation) for rendered concept path. Example: /NXarpes/ENTRY/INSTRUMENT/analyser None <code>--help</code> boolean Show this message and exit. <code>False</code> <p>NOTE: Only one option from (<code>-d</code> and <code>-c</code>) is acceptable.</p>"},{"location":"reference/cli-api.html#eln-generation","title":"ELN generation","text":""},{"location":"reference/cli-api.html#generate_eln","title":"generate_eln","text":"<p>Helper tool for generating ELN files in YAML format.</p> <p>Usage:</p> <pre><code>generate_eln [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--nxdl</code> text Name of NeXus definition without extension (.nxdl.xml). _required <code>--skip-top-levels</code> integer To skip the level of parent hierarchy level. E.g. for default 1 the part Entry[ENTRY] from /Entry[ENTRY]/Instrument[INSTRUMENT]/... will be skiped. <code>1</code> <code>--output-file</code> text Name of file that is neede to generated output file. <code>eln_data</code> <code>--eln-type</code> choice (<code>eln</code> | <code>scheme_eln</code>) Choose a type of ELN output (eln or scheme_eln). <code>eln</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/definitions.html","title":"NeXus definitions","text":"<p>We link two references here. The first links to the official definitions by the NIAC and the second one links to latest FAIRmat definitions.</p> <ul> <li>Official NIAC definitions</li> <li>Latest FAIRmat definitions</li> </ul> <p>The FAIRmat definitions are regularly contributed to NIAC (around every 6 months) but generally reflect a state which is still under development and may contain new or improved application definitions or base classes. Consider it as the public review stage of these application definitions. However, there might be some parts which are still under discussion and will be subject to change.</p> <p>Note: To connect NeXus concepts with semantic web tools, efforts are underway to represent them using the W3C Web Ontology Language (OWL). See the NeXusOntology for more details.</p>"},{"location":"reference/plugins.html","title":"Plugins","text":"<p>There are a number of plugins available for pynxtools that are maintained within FAIRmat. These are extensions of pynxtools used for reading data of specific experimental techniques.</p>"},{"location":"reference/plugins.html#photoemission-spectroscopy","title":"Photoemission spectroscopy","text":"<ul> <li>pynxtools-mpes: A reader for multi-dimensional photoelectron spectroscopy (MPES) data.</li> <li>pynxtools-xps: A reader for X-ray photoelectron spectroscopy (XPS) data from various vendors. Documentation can be found here.</li> </ul>"},{"location":"reference/plugins.html#electron-microscopy","title":"Electron microscopy","text":"<ul> <li>pynxtools-em: A reader for electron microscopy data from various vendors. Documentation can be found here.</li> </ul>"},{"location":"reference/plugins.html#atom-probe-tomography","title":"Atom probe tomography","text":"<ul> <li>pynxtools-apm: A reader for atom probe as well as related field ion microscopy data. Documentation can be found here.</li> </ul>"},{"location":"reference/plugins.html#optical-spectroscopy","title":"Optical spectroscopy","text":"<ul> <li>pynxtools-ellips: A reader for ellipsometry data. Documentation can be found here.</li> <li>pynxtools-raman: A reader for Raman data.</li> </ul>"},{"location":"reference/plugins.html#scanning-probe-microscopy","title":"Scanning probe microscopy","text":"<ul> <li>pynxtools-stm: A reader for scanning tunneling microscopy (SPM) and spectroscopy (STS) data.</li> </ul>"},{"location":"reference/plugins.html#x-ray-diffraction","title":"X-ray diffraction","text":"<ul> <li>pynxtools-xrd: A reader for X-ray diffraction data.</li> </ul>"},{"location":"reference/plugins.html#others","title":"Others","text":""},{"location":"reference/plugins.html#installation","title":"Installation","text":"<p>You can install each of the plugins together with pynxtools by passing the name of the plugin as an extra to the pip install call. For example, for the <code>pynxtools-mpes</code> plugin:</p> <pre><code>pip install pynxtools[mpes]\n</code></pre> <p>In addition, you can also install all of the pynxtools reader plugins which are maintained by FAIRmat by passing the <code>[convert]</code> extra to the pip install call:</p> <pre><code>pip install pynxtools[convert]\n</code></pre>"},{"location":"tutorial/converting-data-to-nexus.html","title":"Converting research data to NeXus","text":""},{"location":"tutorial/converting-data-to-nexus.html#who-is-this-tutorial-for","title":"Who is this tutorial for?","text":"<p>The document is for people who want to standardize their research data by converting their research data into a NeXus standardized format. We cover the basic principles and common principles of NeXus, here. For a more detailed description on the general principles of NeXus we recommend reading our learning page for NeXus or the official NeXus user manual.</p>"},{"location":"tutorial/converting-data-to-nexus.html#what-should-you-should-know-before-this-tutorial","title":"What should you should know before this tutorial?","text":"<ul> <li>You should have a basic understanding of NeXus - A primer on NeXus</li> <li>You should have a basic understanding of FAIR data</li> </ul>"},{"location":"tutorial/converting-data-to-nexus.html#what-you-will-know-at-the-end-of-this-tutorial","title":"What you will know at the end of this tutorial?","text":"<p>You will have</p> <ul> <li>a basic understanding how to use the NeXus data converter from the pynxtools package</li> </ul>"},{"location":"tutorial/converting-data-to-nexus.html#setup","title":"Setup","text":"<p>We use a Python tool to make converting our research data easier. This has a number of readers that support multiple file formats. You can browse the separate folders to find the reader that might work for you. A generic reader is the JSON Map Reader. In addition, we provide multiple reader plugins for different experimental techniques.</p> <p>We will use the XPS reader plugin with a SpecsLabProdigy file (file extension: .sle) as an example.</p>"},{"location":"tutorial/converting-data-to-nexus.html#steps","title":"Steps","text":"<ol> <li>Download the example files from here: Example files</li> <li>Extract the zip and copy the files in your current working directory. You can find the working directory by typing the following in your terminal: <pre><code>pwd\n</code></pre></li> <li>Install pynxtools with the XPS reader plugin: <pre><code>pip install pynxtools[xps]\n</code></pre></li> <li>Verify you can run the <code>dataconverter</code> in a terminal window. Open a terminal with the Python environment where you installed <code>pynxtools</code>. Then type the following: <pre><code>dataconverter --help\n</code></pre></li> </ol>"},{"location":"tutorial/converting-data-to-nexus.html#converting-the-example-files","title":"Converting the example files","text":"<p>Once you have your files copied into the working directory, your directory structure should look like this: <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 EX439_S718_Au.sle\n\u251c\u2500\u2500 eln_data_sle.yaml\n\u2514\u2500\u2500 params.yaml\n</code></pre></p> <p>The <code>eln_data_sle.yaml</code> YAML file is another data file containing additional information (e.g., information about the experimentator) that is not provided in the main data file.</p> <p>Next, you will run the conversion routine from your Python environment: <pre><code>dataconverter --params-file params.yaml\n</code></pre></p> <p>Here we use a <code>params.yaml</code> parameter file to configure the converter.  This will create a file called <code>Au_25_mbar_O2_no_align.nxs</code> in your current directory.</p> <p>Congrats! You now have a FAIR NeXus file!</p> <p>You can try out other examples from pynxtools.</p>"},{"location":"tutorial/nexus-to-nomad.html","title":"Uploading NeXus files to NOMAD","text":"<p>Great choice! Nomad makes it easier than ever to work with your research data. At this point you are probably have an idea of what FAIR data is. Even if you don't, it doesn't matter. Nomad provides a simple graphical interface that let's you collect and have your data ready for publication.</p> <p>In this tutorial, we will go through how one can upload their NeXus files to Nomad.</p> <p>Nomad, as a FAIR data platform, supports NeXus and allows users to upload their NeXus (.nxs) files directly. These files get interpreted and added to your Nomad account with complete control on how you would like to present and publish them alongside your research.</p>"},{"location":"tutorial/nexus-to-nomad.html#create-an-account","title":"Create an account","text":"<p>The very first thing you need to do is get a Nomad account. If you don't have one you can register here.</p> <ol> <li>Navigate to nomad-lab.eu</li> <li>Click on <code>Login / Register</code> on the top right corner.</li> </ol> <p></p>"},{"location":"tutorial/nexus-to-nomad.html#create-an-upload","title":"Create an Upload","text":"<p>Nomad allows you to have a draft working space called an upload. This allows you to test and prepare how your data will look in Nomad before you publish it.</p> <p>Go to <code>Publish -&gt; Uploads</code></p> <p></p> <p></p> <p>Click <code>Create a new upload</code></p> <p></p>"},{"location":"tutorial/nexus-to-nomad.html#upload-your-nexus-file","title":"Upload your NeXus file","text":"<p>Now we can upload your FAIR NeXus file and let Nomad interpret it for us.</p> <p>Click the <code>Drop files here...</code> button and choose your NeXus file from your device. </p> <p>Once Nomad has interpreted your data, this is what your screen will look like.</p> <p></p>"},{"location":"tutorial/nexus-to-nomad.html#browsing-your-nexus-data","title":"Browsing your NeXus data","text":"<p>You can find the Nomad interpretation of your data under entries. If you click on this arrow, you will be able to see an Overview of your NeXus file.</p> <p></p> <p></p> <p>On the Overview page you will be presented with <code>H5Web</code> that let's you browse the data in your <code>NeXus</code> file directly.</p> <p></p> <p></p> <p>Nomad also interprets and <code>normalizes</code> this data to make it interoperable with other corners of Material's research. To browse this <code>normalized</code> data you can browse the <code>DATA</code> tab. Here you see all the information Nomad has picked up and made available for search and comparison with synthesis, experimental, and computational materials data.</p> <p></p> <p>Feel free to explore more!</p>"}]}