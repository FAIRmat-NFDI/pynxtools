{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nomad-parser-nexus demo for Multidimensional Photoemission Spectroscopy (MPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook displaying parsing from FHI preprocessing software output (x-array based .h5) to NXmpes formatted nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step -1:** Create a new virtual environment and set up dependencies for jupyterlab_h5web.<br> This section **should only be run if you do not have Jupyter Lab, its extensions and the extra needed packages.** <br>  For use within the Nomad UI this section should be skipped.<br> These cells can be run in a Jupyter notebook or copied to a terminal without the \"!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to start with a fresh virtualenv in **your terminal** in which you are installing and then running **Jupyter lab** if you wish to: <br>\n",
    "```pip install virtualenv && virtualenv --python=python3.7 .nexusenv && source .nexusenv/bin/activate```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install jupyter, jupyter-lab and h5web extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade nodejs && pip install ipywidgets h5py==3.5.0 h5glance==0.7 h5grove==0.0.14 jupyterlab[full]==3.2.9 jupyterlab_h5web[full]==1.3.0 punx==0.2.5 nexpy==0.14.1 silx[full]==1.0.0 && jupyter lab build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable the extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! jupyter serverextension enable jupyterlab_h5web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step 0:** Installing and testing nomad-parser-nexus module. <br> This section **should only be run if you are not running this within NOMAD.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install nomad and its dependencies. Do not run the following cell if you have a nomad installation running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip && pip install nomad-lab==1.0.0 --extra-index-url https://gitlab.mpcdf.mpg.de/api/v4/projects/2187/packages/pypi/simple\n",
    "! pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install mpes and its dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/rettigl/mpes.git\n",
    "! pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the nexusparser and its requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Arora0/nomad-parser-nexus.git --recursive && cd nomad-parser-nexus && git checkout dataconverter-baseclasses-check --recurse-submodules && git status && pip install -r requirements.txt && pip install -e .[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell restarts the kernel after the nexusparser installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step 1:** Download example RAW data (trARPES data of WSe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check dependencies of nomad and of the nexusparser and prints them. If the nexusparser and nomad-lab are installed, you are ready to go.<br> Check if jupyterlab_h5web server and lab extensions are enabled and OK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list | grep 'nomad\\|nexus'\n",
    "! jupyter serverextension list\n",
    "! jupyter labextension list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download mpes example data from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "! curl --output WSe2.zip https://zenodo.org/record/6369728/files/WSe2.zip\n",
    "shutil.unpack_archive('WSe2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpes import base, fprocessing as fp, analysis as aly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from dask import compute\n",
    "import datetime as dt\n",
    "import h5py\n",
    "fdir = r'Scan049_1'\n",
    "outdir = r'Scan049_binned/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data binning for distortion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parp = fp.parallelHDF5Processor(folder=fdir)\n",
    "parp.gather(identifier=r'/*.h5', file_sorting=True)\n",
    "len(parp.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parp.files = parp.files[0:50]\n",
    "axes = ['X', 'Y', 't']\n",
    "# Important to keep the whole detector area for the initial binning!\n",
    "bins = [512, 512, 300]\n",
    "ranges = [(0, 2048), (0, 2048), (64000, 68000)]\n",
    "parp.parallelBinning(axes=axes, nbins=bins, ranges=ranges, scheduler='threads', ret=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine correction landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = aly.MomentumCorrector(parp.combinedresult['binned'])\n",
    "mc.selectSlice2D(slice(165, 175), 2)\n",
    "mc.featureExtract(mc.slice, sigma=5, fwhm=10, sigma_radius=3)\n",
    "mc.view(points=mc.features, annotated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate thin plate spline symmetry correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.splineWarpEstimate(image=mc.slice, landmarks=mc.pouter_ord, include_center=True,\n",
    "                      iterative=False, interp_order=2, update=True)\n",
    "mc.view(image=mc.slice_transformed, annotated=True, points={'feats':mc.ptargs}, backend='bokeh', crosshair=True, radii=[75,110,150], crosshair_thickness=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.coordinateTransform(type='translation', xtrans=70, ytrans=70, keep=True)\n",
    "plt.imshow(mc.slice_transformed, origin='lower', cmap='terrain_r')\n",
    "plt.axvline(x=256)\n",
    "plt.axhline(y=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.coordinateTransform( type='rotation', angle=-5, center=(256., 256.), keep=True)\n",
    "plt.imshow(mc.slice_transformed, origin='lower', cmap='terrain_r')\n",
    "plt.axvline(x=256)\n",
    "plt.axhline(y=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Deformation field:\n",
    "subs = 20\n",
    "plt.scatter(mc.cdeform_field[::subs,::subs].ravel(), mc.rdeform_field[::subs,::subs].ravel(), c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one high-symmetry point\n",
    "point_b = [252.,255.]\n",
    "# Pick the BZ center\n",
    "point_a = [308.,346.]\n",
    "# give the distance of the two in inverse Angstrom\n",
    "distance = np.pi*4/3/3.297\n",
    "# Momentum calibration assuming equal scaling along both x and y directions (equiscale=True)\n",
    "# Requirements : pixel coordinates of and the momentum space distance between two symmetry points, plus the momentum coordinates\n",
    "# of one of the two points \n",
    "ext = mc.calibrate(mc.slice_transformed, point_from=point_a, point_to=point_b, dist=distance, equiscale=True, ret=['extent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.view(image=mc.slice_transformed, imkwds=ext)\n",
    "plt.xlabel('$k_x$', fontsize=15)\n",
    "plt.ylabel('$k_y$', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = ['t']\n",
    "bins = [1000]\n",
    "ranges = [(63000, 80000)]\n",
    "traces, tof = fp.extractEDC(folder=r'energycal_2019_01_08', axes=axes, bins=bins, ranges=ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voltages = np.arange(-12.2, -23.2, -1)\n",
    "ec = aly.EnergyCalibrator(biases=voltages, traces=traces, tof=tof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.normalize(smooth=True, span=7, order=1)\n",
    "ec.view(traces=ec.traces_normed, xaxis=ec.tof, backend='bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = [(65000, 65200)]\n",
    "ec.addFeatures(traces=ec.traces_normed, refid=0, ranges=rg[0], infer_others=True, mode='append')\n",
    "ec.featranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.featureExtract(traces=ec.traces_normed, ranges=ec.featranges)\n",
    "ec.view(traces=ec.traces_normed, peaks=ec.peaks, backend='bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate energy calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refid=5\n",
    "Eref=-1.3\n",
    "axs = ec.calibrate(ret='all', Eref=Eref, t=ec.tof, refid=refid)\n",
    "ec.view(traces=ec.traces_normed, xaxis=ec.calibration['axis'], backend='bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality of calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(voltages)):\n",
    "    plt.plot(ec.calibration['axis']-(voltages[i]-voltages[refid]), ec.traces_normed[i])\n",
    "plt.xlim([-15,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect calibration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.view(traces=ec.calibration['axis'][None,:], xaxis=ec.tof, backend='matplotlib', show_legend=False)\n",
    "plt.scatter(ec.peaks[:,0], ec.biases-ec.biases[refid]+Eref, s=50, c='k')\n",
    "plt.xlabel('Time-of-flight', fontsize=15)\n",
    "plt.ylabel('Energy (eV)', fontsize=15)\n",
    "plt.ylim([-8,6])\n",
    "plt.xlim([63400,69800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = fp.dataframeProcessor(datafolder=fdir)\n",
    "# Read events in with ms time stamps\n",
    "dfp.read(source='folder', ftype='h5', timeStamps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply energy calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.appendEAxis(E0=ec.calibration['E0'], a=ec.calibration['coeffs'])\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply distortion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.applyKCorrection(type='tps_matrix', rdeform_field = mc.rdeform_field, cdeform_field = mc.cdeform_field, X='X', Y='Y', newX='Xm', newY='Ym')\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply momentum calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.appendKAxis(point_b[0], point_b[1], X='Xm', Y='Ym', rstart=parp.binranges[0][0], cstart=parp.binranges[1][0], \\\n",
    "                rstep=parp.binsteps[0], cstep=parp.binsteps[1], fc=mc.calibration['coeffs'][0], fr=mc.calibration['coeffs'][1])\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply pump-probe delay axis conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADCRange = (650, 6900)\n",
    "timeRange = (-100, 200)\n",
    "dfp.edf['delay'] = timeRange[0] + (dfp.edf['ADC']-ADCRange[0]) * (timeRange[1] - timeRange[0])/(ADCRange[1]-ADCRange[0])\n",
    "dfp.edf.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin 4D data in transformed grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = ['kx', 'ky', 'E', 'delay']\n",
    "bins = [50, 50, 100, 21]\n",
    "ranges = [(-2, 2), (-2, 2), (-3, 2), (-110, 190)]\n",
    "# jittering of energy and ADC should best be done on the bin size of the hardware, not the rebinned bin size\n",
    "TOFrange=[64500,67000]\n",
    "e_t_conversion = (base.tof2evpoly(ec.calibration['coeffs'], ec.calibration['E0'], TOFrange[0]) - base.tof2evpoly(ec.calibration['coeffs'], ec.calibration['E0'], TOFrange[1]))/(TOFrange[1]-TOFrange[0])\n",
    "d_adc_conversion = (timeRange[1]-timeRange[0])/(ADCRange[1]-ADCRange[0])\n",
    "jitter_amplitude = [0.5, 0.5, 1*bins[2]/abs(ranges[2][1]-ranges[2][0])*e_t_conversion, 1*bins[3]/abs(ranges[3][1]-ranges[3][0])*d_adc_conversion]\n",
    "dfp.distributedBinning(axes=axes, nbins=bins, ranges=ranges, scheduler='threads', ret=False, jittered=True, jitter_amplitude=jitter_amplitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create metatada structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time stamps from dataframe\n",
    "dfpart = dfp.edf.get_partition(0)\n",
    "all_data = np.array(compute(dfpart.values))[0,:,:]\n",
    "timeStamps = all_data[:,6]\n",
    "tsFrom = timeStamps[0]\n",
    "dfpart = dfp.edf.get_partition(len(dfp.datafiles)-1)\n",
    "all_data = np.array(compute(dfpart.values))[0,:,:]\n",
    "timeStamps = all_data[:,6]\n",
    "tsTo = timeStamps[len(timeStamps)-1]\n",
    "metadata['timing'] = {'acquisition_start': dt.datetime.utcfromtimestamp(tsFrom/1000).replace(tzinfo=dt.timezone.utc).isoformat(),\n",
    "                      'acquisition_stop': dt.datetime.utcfromtimestamp(tsTo/1000).replace(tzinfo=dt.timezone.utc).isoformat(),\n",
    "                      'acquisition_duration': int((tsTo - tsFrom)/1000),\n",
    "                      'bin_array_creation': dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat(),\n",
    "                      'collection_time': float((tsTo - tsFrom)/1000)\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import meta data from data file\n",
    "file0 = parp.files[0]\n",
    "\n",
    "metadata['file'] = {}\n",
    "with h5py.File(file0, 'r') as f:\n",
    "    for k,v in f.attrs.items():\n",
    "        metadata['file'][k]=v      \n",
    "        \n",
    "metadata['file']['KTOF:Lens:Extr:I'] = 0.\n",
    "metadata['file']['KTOF:Lens:UDLD:VSet'] = 400.\n",
    "metadata['file']['KTOF:Lens:Sample:VSet'] = 17.\n",
    "\n",
    "metadata['entry_identifier'] = fdir[13:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta data of the binning\n",
    "momentum_dict = mc.__dict__.copy()\n",
    "energy_dict = ec.__dict__.copy()\n",
    "binning_dict = dfp.__dict__.copy()\n",
    "\n",
    "momentum_dict['calibration']['coeffs'] = np.array(momentum_dict['calibration']['coeffs'])\n",
    "momentum_dict['adjust_params']['center'] = np.array(momentum_dict['adjust_params']['center'])\n",
    "momentum_dict['pcent'] = np.array(momentum_dict['pcent'])\n",
    "# to reduce the size of h5 file for upload on github\n",
    "momentum_dict.pop('image')\n",
    "binning_dict.pop('histdict')\n",
    "binning_dict.pop('dfield')\n",
    "\n",
    "metadata['energy_correction'] = energy_dict\n",
    "metadata['momentum_correction'] = momentum_dict\n",
    "metadata['binning'] = binning_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual Meta data\n",
    "#General\n",
    "metadata['experiment_summary'] = 'WSe2 XUV NIR pump probe data.'\n",
    "metadata['entry_title'] = 'Valence Band Dynamics - 800 nm linear s-polarized pump, 0.6 mJ/cm2 absorbed fluence'\n",
    "metadata['experiment_title'] = 'Valence band dynamics of 2H-WSe2'\n",
    "\n",
    "#User\n",
    "# Fill general parameters of NXuser\n",
    "# TODO: discuss how to deal with multiple users?\n",
    "metadata['user0'] = {}\n",
    "metadata['user0']['name'] = 'Julian Maklar'\n",
    "metadata['user0']['role'] = 'Principal Investigator'\n",
    "metadata['user0']['affiliation'] = 'Fritz Haber Institute of the Max Planck Society'\n",
    "metadata['user0']['address'] = 'Faradayweg 4-6, 14195 Berlin'\n",
    "metadata['user0']['email'] = 'maklar@fhi-berlin.mpg.de'\n",
    "\n",
    "#NXinstrument\n",
    "metadata['instrument'] = {}\n",
    "#analyzer\n",
    "metadata['instrument']['analyzer']={}\n",
    "metadata['instrument']['analyzer']['slow_axes'] = \"delay\" # the scanned axes\n",
    "metadata['instrument']['analyzer']['spatial_resolution'] = 10.\n",
    "metadata['instrument']['analyzer']['energy_resolution'] = 110.\n",
    "metadata['instrument']['analyzer']['momentum_resolution'] = 0.08\n",
    "metadata['instrument']['analyzer']['projection'] = \"reciprocal\"\n",
    "metadata['instrument']['analyzer']['working_distance'] = 4.\n",
    "metadata['instrument']['analyzer']['lens_mode'] = \"6kV_kmodem4.0_30VTOF.sav\"\n",
    "\n",
    "# Need to get those from the data file...\n",
    "# Put into separate Lens objects?\n",
    "metadata['instrument']['analyzer']['fa_size'] = 200\n",
    "metadata['instrument']['analyzer']['ca_size'] = np.nan\n",
    "\n",
    "#probe beam\n",
    "metadata['instrument']['beam']={}\n",
    "metadata['instrument']['beam']['probe']={}\n",
    "metadata['instrument']['beam']['probe']['incident_energy'] = 21.7\n",
    "metadata['instrument']['beam']['probe']['incident_energy_spread'] = 0.11\n",
    "metadata['instrument']['beam']['probe']['pulse_duration'] = 20.\n",
    "metadata['instrument']['beam']['probe']['frequency'] = 500.\n",
    "metadata['instrument']['beam']['probe']['incident_polarization'] = [1, 1, 0, 0] # p pol Stokes vector\n",
    "metadata['instrument']['beam']['probe']['extent'] = [80., 80.] \n",
    "#pump beam\n",
    "metadata['instrument']['beam']['pump']={}\n",
    "metadata['instrument']['beam']['pump']['incident_energy'] = 1.55\n",
    "metadata['instrument']['beam']['pump']['incident_energy_spread'] = 0.08\n",
    "metadata['instrument']['beam']['pump']['pulse_duration'] = 35.\n",
    "metadata['instrument']['beam']['pump']['frequency'] = 500.\n",
    "metadata['instrument']['beam']['pump']['incident_polarization'] = [1, -1, 0, 0] # s pol Stokes vector\n",
    "metadata['instrument']['beam']['pump']['incident_wavelength'] = 800. \n",
    "metadata['instrument']['beam']['pump']['average_power'] = 300.\n",
    "metadata['instrument']['beam']['pump']['pulse_energy'] = metadata['instrument']['beam']['pump']['average_power']/metadata['instrument']['beam']['pump']['frequency']#ÂµJ\n",
    "metadata['instrument']['beam']['pump']['extent'] = [230., 265.] \n",
    "metadata['instrument']['beam']['pump']['fluence'] = 0.15\n",
    "\n",
    "#sample\n",
    "metadata['sample']={}\n",
    "metadata['sample']['preparation_date'] = '2019-01-13T10:00:00+00:00'\n",
    "metadata['sample']['sample_history'] = 'Cleaved'\n",
    "metadata['sample']['chemical_formula'] = 'WSe2'\n",
    "metadata['sample']['description'] = 'Sample'\n",
    "metadata['sample']['name'] = 'WSe2 Single Crystal'\n",
    "metadata['sample']['temperature'] = 300.\n",
    "metadata['sample']['pressure'] = 5.e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for generating xarrays from the data, and saving as intermediary h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "default_units = {\n",
    "    'X': 'step', \n",
    "    'Y': 'step', \n",
    "    't': 'step', \n",
    "    'tofVoltage':'V',\n",
    "    'extractorVoltage':'V',\n",
    "    'extractorCurrent':'A',\n",
    "    'cryoTemperature':'K',\n",
    "    'sampleTemperature':'K',\n",
    "    'dldTimeBinSize':'ns',\n",
    "    'delay':'fs',\n",
    "    'timeStamp':'s',\n",
    "    'E':'eV',\n",
    "    'energy':'eV',\n",
    "    'kx':'1/A',\n",
    "    'ky':'1/A'}\n",
    "\n",
    "def res_to_xarray(res, binNames, binAxes, metadata=None):\n",
    "    \"\"\" creates a BinnedArray (xarray subclass) out of the given np.array\n",
    "    Parameters:\n",
    "        res: np.array\n",
    "            nd array of binned data\n",
    "        binNames (list): list of names of the binned axes\n",
    "        binAxes (list): list of np.arrays with the values of the axes\n",
    "    Returns:\n",
    "        ba: BinnedArray (xarray)\n",
    "            an xarray-like container with binned data, axis, and all available metadata\n",
    "    \"\"\"\n",
    "    dims = binNames\n",
    "    coords = {}\n",
    "    for name, vals in zip(binNames, binAxes):\n",
    "        coords[name] = vals\n",
    "\n",
    "    xres = xr.DataArray(res, dims=dims, coords=coords)\n",
    "\n",
    "    for name in binNames:\n",
    "        try:\n",
    "            xres[name].attrs['unit'] = default_units[name]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    xres.attrs['units'] = 'counts'\n",
    "    xres.attrs['long_name'] = 'photoelectron counts'\n",
    "\n",
    "    if metadata is not None:\n",
    "        xres.attrs['metadata'] = metadata\n",
    "\n",
    "    return xres\n",
    "\n",
    "import h5py\n",
    "def xarray_to_h5(data, faddr, mode='w'):\n",
    "    \"\"\" Save xarray formatted data to hdf5\n",
    "    Args:\n",
    "        data (xarray.DataArray): input data\n",
    "        faddr (str): complete file name (including path)\n",
    "        mode (str): hdf5 read/write mode\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    with h5py.File(faddr, mode) as h5File:\n",
    "\n",
    "        print(f'saving data to {faddr}')\n",
    "\n",
    "        # Saving data\n",
    "\n",
    "        ff = h5File.create_group('binned')\n",
    "\n",
    "        # make a single dataset\n",
    "        ff.create_dataset('BinnedData', data=data.data)\n",
    "\n",
    "        # Saving axes\n",
    "        aa = h5File.create_group(\"axes\")\n",
    "        # aa.create_dataset('axis_order', data=data.dims)\n",
    "        ax_n = 0\n",
    "        for binName in data.dims:\n",
    "            ds = aa.create_dataset(f'ax{ax_n}', data=data.coords[binName])\n",
    "            ds.attrs['name'] = binName\n",
    "            ax_n += 1\n",
    "\n",
    "\n",
    "        if ('metadata' in data.attrs and isinstance(data.attrs['metadata'], dict)):\n",
    "            meta_group = h5File.create_group('metadata')\n",
    "            \n",
    "            def recursive_write_metadata(h5group, node):\n",
    "                for key, item in node.items():\n",
    "                    if isinstance(item, (np.ndarray, np.int64, np.float64, str, bytes, int, float, list)):\n",
    "                        try:\n",
    "                            h5group.create_dataset(key, data=item)\n",
    "                        except TypeError:\n",
    "                            h5group.create_dataset(key, data=str(item))\n",
    "                            print(\"saved \" + key + \" as string\")\n",
    "                    elif isinstance(item, dict):\n",
    "                        print(key)\n",
    "                        group = h5group.create_group(key)\n",
    "                        recursive_write_metadata(group, item)\n",
    "                    else:\n",
    "                        try:\n",
    "                            h5group.create_dataset(key, data=str(item))\n",
    "                            print(\"saved \" + key + \" as string\")\n",
    "                        except:\n",
    "                            raise ValueError('Cannot save %s type'%type(item))\n",
    "            \n",
    "            \n",
    "     \n",
    "            recursive_write_metadata(meta_group, data.attrs['metadata'])\n",
    "                   \n",
    "    print('Saving complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert and save as h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axnames = dfp.binaxes.copy()\n",
    "axnames[2] = \"energy\"\n",
    "axes = [dfp.histdict[ax] for ax in dfp.binaxes]\n",
    "res_xarray = res_to_xarray(dfp.histdict['binned'], axnames, axes, metadata)\n",
    "xarray_to_h5(res_xarray, \"WSe2_xarray.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step 2:** Run your mpes-specific parser on the example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the nexusparser directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nexusparser\n",
    "nexus_dir = os.path.dirname(nexusparser.__file__)  # where the nexusparser module is located!!!!\n",
    "print(nexus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our parser. The --reader flag takes the mpes reader (mpes), the --nxdl flag takes the application definition for this technique.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python \"{nexus_dir}/tools/dataconverter/convert.py\" \\\n",
    "--reader mpes \\\n",
    "--nxdl NXmpes \\\n",
    "--input-file WSe2_xarray.h5 \\\n",
    "--input-file config_file.json \\\n",
    "--input-file ELN_metadata_example.yaml \\\n",
    "--output WSe2.mpes.nxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key take home message is that the command above-specified triggers the automatic creation of the HDF5 file.<br>\n",
    "This *.nxs file, is an HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the same parser alternatively using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexusparser.tools.dataconverter.convert import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(input_file=[\"config_file.json\",\"ELN_metadata_example.yaml\"],\n",
    "        reader='mpes',\n",
    "        nxdl='NXmpes',\n",
    "        output='WSe2.mpes.nxs',\n",
    "        objects=(res_xarray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step 3:** Inspect the HDF5/NeXus file WSe2.mpes.nxs using H5Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterlab_h5web import H5Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_name = 'WSe2.mpes.nxs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H5Web(h5_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
